{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_BATCH_SIZE = 60\n",
    "VAL_BATCH_SIZE = 30\n",
    "DATA_PATH = '../data/Nottingham/'\n",
    "NUM_EPOCHS = 5\n",
    "POSITIVE_WEIGHT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_pianoroll(path, poisson=True, resolution = 4):\n",
    "    #Resolution is set to 3 so that the sequences are not that long\n",
    "    midi_data = pypianoroll.read(path, resolution=resolution)\n",
    "    \n",
    "    piano_roll = midi_data.blend()[:, 21:109] #Taking just 81 usefull notes\n",
    "    \n",
    "    #we want to perform multilabel classification at each step so we need to binaryze the roll\n",
    "    piano_roll[piano_roll > 0] = 1\n",
    "    \n",
    "    if poisson:\n",
    "        current_roll = piano_roll[np.newaxis,0, :] # to have shape (1, num_of_notes)\n",
    "        count = 1\n",
    "        counts = []\n",
    "        new_piano_roll = current_roll\n",
    "        for i in range(1, piano_roll.shape[0]):\n",
    "            next_roll = piano_roll[np.newaxis, i, :]\n",
    "            if np.all(current_roll == next_roll):\n",
    "                count += 1\n",
    "            else:\n",
    "                counts.append(count)\n",
    "                count = 1\n",
    "                \n",
    "                new_piano_roll = np.concatenate((new_piano_roll, next_roll), axis=0)\n",
    "                \n",
    "                current_roll = next_roll\n",
    "                \n",
    "        counts.append(count)\n",
    "        new_piano_roll = np.concatenate((new_piano_roll, np.array(counts)[:,np.newaxis]), axis=1)\n",
    "        return new_piano_roll \n",
    "                           \n",
    "    return piano_roll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_path = os.path.join(DATA_PATH, \"train\", \"ashover_simple_chords_21.mid\")\n",
    "\n",
    "roll = path_to_pianoroll(midi_path, False,resolution = 8)\n",
    "\n",
    "roll2 = path_to_pianoroll(midi_path, True,resolution = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 88)\n",
      "(294, 89)\n",
      "[2 1 1 1 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2\n",
      " 2 1 7 1 2 1 1 1 2 1 7 1 2 1 1 1 2 1 5 2 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1 2 1\n",
      " 1 1 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2 2 1\n",
      " 7 1 2 1 1 1 2 1 7 1 2 1 1 1 2 1 5 2 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1 2 1 1 1\n",
      " 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2 2 1 7 1\n",
      " 2 1 1 1 2 1 7 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1 2 1 1\n",
      " 1 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2 2 1 7\n",
      " 1 2 1 1 1 2 1 7 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1]\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(roll.shape)\n",
    "print(roll2.shape) # This has less timesteps and the difference will be bigger w.r.t resolution, and this has one additional entry in 2ndim dim which are counts\n",
    "print(roll2[:,-1])\n",
    "print(np.sum(roll2[:,-1])) # Now the improvement is much bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    #Helper function for DataLoader\n",
    "    #Batch is a list of tuple in the form (input, target)\n",
    "    #We do not have to padd everything thanks to pack_sequence\n",
    "    data = [item[0] for item in batch] #\n",
    "    data = nn.utils.rnn.pack_sequence(data, enforce_sorted=False)\n",
    "    targets = [item[1] for item in batch]\n",
    "    targets = nn.utils.rnn.pack_sequence(targets, enforce_sorted=False)\n",
    "    return [data, targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesGenerationDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, path,):\n",
    "        \n",
    "        self.path = path\n",
    "        self.full_filenames = []\n",
    "        \n",
    "        #Here we assume that all midi files are valid, we do not check anything here.\n",
    "        for root, subdirs, files in os.walk(path):\n",
    "            for f in files:\n",
    "                self.full_filenames.append(os.path.join(root, f))\n",
    "                    \n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.full_filenames)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        full_filename = self.full_filenames[index]\n",
    "        \n",
    "        piano_roll = path_to_pianoroll(full_filename)\n",
    "        \n",
    "        #input and gt are shifted by one step w.r.t one another.\n",
    "        #we transpose it since piano_roll has shape [num_of_notes, number_of_event] we want to have format [number of events, num_of_notes]\n",
    "        input_sequence = piano_roll[:, :-1]\n",
    "        ground_truth_sequence = piano_roll[:, 1:]\n",
    "        \n",
    "        return torch.tensor(input_sequence, dtype=torch.float32), torch.tensor(ground_truth_sequence, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NotesGenerationDataset(os.path.join(DATA_PATH, \"train\"))\n",
    "\n",
    "#ofc we want big batch_size. However, one training sample takes quite a lot of memory.\n",
    "#We will use torch.cuda.amp.autocast() so that we can make bigger batches\n",
    "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE,\n",
    "                                              shuffle=True, drop_last=True, collate_fn=collate)\n",
    "\n",
    "valset = NotesGenerationDataset(DATA_PATH, train=False)\n",
    "\n",
    "valset_loader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a2214deb2e00a4588fb64d6e2ad9e78ab07788ce628f39696990503e7a4b014"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
