{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved first baseline. Repetitive nature of piano rolls is used here. Since it's quite common to play longer notes it usually hapens that few timestep next to one another are equal. We can count how many times in a row a timestep repeats and then write the timestep with additional dimension for number of repetitions. Using this technique we shorten sequences by much. To predict counts we can use Poisson regression. This notebook is very similar to baseline.ipynb I didn't want to merge them it one module for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_BATCH_SIZE = 30\n",
    "VAL_BATCH_SIZE = 30\n",
    "DATA_PATH = '../data/Nottingham/'\n",
    "NUM_EPOCHS = 5\n",
    "POSITIVE_WEIGHT = 2\n",
    "CLIP_VALUE = 1.0 # clip value for the gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_pianoroll(path, poisson=True, resolution = 8):\n",
    "    midi_data = pypianoroll.read(path, resolution=resolution)\n",
    "    \n",
    "    piano_roll = midi_data.blend()[:, 21:109] #Taking just 88 usefull notes\n",
    "    \n",
    "    #we want to perform multilabel classification at each step so we need to binaryze the roll\n",
    "    piano_roll[piano_roll > 0] = 1\n",
    "    \n",
    "    # here with poisson we iterate over entire pianoroll and count how many times timesteps repeats\n",
    "    if poisson:\n",
    "        current_roll = piano_roll[np.newaxis,0, :] # to have shape (1, num_of_notes)\n",
    "        count = 1\n",
    "        counts = []\n",
    "        new_piano_roll = current_roll\n",
    "        for i in range(1, piano_roll.shape[0]):\n",
    "            next_roll = piano_roll[np.newaxis, i, :] # newaxis so that dimensions match\n",
    "            if np.all(current_roll == next_roll):\n",
    "                #if two rolls next to one another are equal we update count\n",
    "                count += 1\n",
    "            else:\n",
    "                #If roll is different we concatenate previous part of piano roll with that new timestep\n",
    "                #and write for how many times it should be repeated\n",
    "                counts.append(count)\n",
    "                count = 1\n",
    "                \n",
    "                new_piano_roll = np.concatenate((new_piano_roll, next_roll), axis=0)\n",
    "                \n",
    "                current_roll = next_roll\n",
    "                \n",
    "        counts.append(count)\n",
    "        #At the very end we concatenate counts with piano roll obtaining (timesteps, num_of_notes + 1) shape\n",
    "        new_piano_roll = np.concatenate((new_piano_roll, np.array(counts)[:,np.newaxis]), axis=1)\n",
    "        return new_piano_roll \n",
    "                           \n",
    "    return piano_roll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_path = os.path.join(DATA_PATH, \"train\", \"ashover_simple_chords_21.mid\")\n",
    "\n",
    "roll = path_to_pianoroll(midi_path, False,resolution = 8)\n",
    "\n",
    "roll2 = path_to_pianoroll(midi_path, True,resolution = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 88)\n",
      "(294, 89)\n",
      "[2 1 1 1 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2\n",
      " 2 1 7 1 2 1 1 1 2 1 7 1 2 1 1 1 2 1 5 2 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1 2 1\n",
      " 1 1 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2 2 1\n",
      " 7 1 2 1 1 1 2 1 7 1 2 1 1 1 2 1 5 2 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1 2 1 1 1\n",
      " 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2 2 1 7 1\n",
      " 2 1 1 1 2 1 7 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1 2 1 1\n",
      " 1 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2 2 1 7\n",
      " 1 2 1 1 1 2 1 7 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1]\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "#to show what the difference is in length of sequences w.r.t compressed and not compressed\n",
    "print(roll.shape)\n",
    "print(roll2.shape) # This has less timesteps and the difference will be bigger w.r.t resolution, and this has one additional entry in 2ndim dim which are counts\n",
    "print(roll2[:,-1])\n",
    "print(np.sum(roll2[:,-1])) # Now the improvement is much bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    #Helper function for DataLoader\n",
    "    #Batch is a list of tuple in the form (input, target)\n",
    "    #We do not have to padd everything thanks to pack_sequence\n",
    "    data = [item[0] for item in batch] #\n",
    "    data = nn.utils.rnn.pack_sequence(data, enforce_sorted=False)\n",
    "    targets = [item[1] for item in batch]\n",
    "    targets = nn.utils.rnn.pack_sequence(targets, enforce_sorted=False)\n",
    "    return [data, targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesGenerationDataset(data.Dataset):\n",
    "    # this works exactly the same, we just have one additional dimension 88 keys to 88 keys + 1 count\n",
    "    def __init__(self, path,):\n",
    "        \n",
    "        self.path = path\n",
    "        self.full_filenames = []\n",
    "        \n",
    "        #Here we assume that all midi files are valid, we do not check anything here.\n",
    "        for root, subdirs, files in os.walk(path):\n",
    "            for f in files:\n",
    "                self.full_filenames.append(os.path.join(root, f))\n",
    "                    \n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.full_filenames)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        full_filename = self.full_filenames[index]\n",
    "        \n",
    "        piano_roll = path_to_pianoroll(full_filename, poisson=True, resolution=8)\n",
    "        \n",
    "        #input and gt are shifted by one step w.r.t one another.\n",
    "        input_sequence = piano_roll[:-1, :]\n",
    "        ground_truth_sequence = piano_roll[1:, :]\n",
    "        \n",
    "        return torch.tensor(input_sequence, dtype=torch.float32), torch.tensor(ground_truth_sequence, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NotesGenerationDataset(os.path.join(DATA_PATH, \"train\"))\n",
    "\n",
    "#ofc we want big batch_size. However, one training sample takes quite a lot of memory.\n",
    "#We will use torch.cuda.amp.autocast() so that we can make bigger batches\n",
    "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE,\n",
    "                                              shuffle=True, drop_last=True, collate_fn=collate)\n",
    "\n",
    "valset = NotesGenerationDataset(os.path.join(DATA_PATH, \"valid\"))\n",
    "\n",
    "valset_loader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small sanity check that our sets do not intersect at any moment\n",
    "train_songs = set(trainset.full_filenames)\n",
    "for song in valset.full_filenames:\n",
    "    assert not song in train_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n"
     ]
    }
   ],
   "source": [
    "print(trainset.__len__())\n",
    "assert len(os.listdir(os.path.join(DATA_PATH, \"train\"))) == trainset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    #This is the same as for our first baseline, just one additional linear layer at the end is added to return counts parameter\n",
    "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size # amount of different notes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #At first we need layer that will encode our vector with only once to better representation\n",
    "        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        #At the end we want to get vector with logits of all notes\n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.poisson_fc = nn.Linear(hidden_size, 1) # I know that I could merge these into one layer, but this is more readable\n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        \n",
    "        if isinstance(inp, nn.utils.rnn.PackedSequence):\n",
    "            #If we have Packed sequence we proceed a little bit differently\n",
    "            batch_sizes = inp.batch_sizes\n",
    "            notes_encoded = self.notes_encoder(inp.data) #PackedSequence.data is a tensor representation of shape [samples, num_of_notes]\n",
    "            rnn_in = nn.utils.rnn.PackedSequence(notes_encoded,batch_sizes) #This is not recommended in PyTorch documentation.\n",
    "            #However this saves a day here. Since otherwise we would have to create padded sequences \n",
    "            outputs, hidden = self.lstm(rnn_in, hidden)\n",
    "            \n",
    "            class_logits = self.logits_fc(outputs.data) #Again we go from packedSequence to tensor.\n",
    "            poisson_logits = self.poisson_fc(outputs.data)\n",
    "            \n",
    "        else:\n",
    "            #If we have tensor at the input this is pretty straightforward\n",
    "            notes_encoded = self.notes_encoder(inp)\n",
    "            outputs, hidden = self.lstm(notes_encoded, hidden)\n",
    "            class_logits = self.logits_fc(outputs)\n",
    "            poisson_logits = self.poisson_fc(outputs)\n",
    "        \n",
    "        return class_logits, poisson_logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now sanity check about Packed Sequences. So I check if Unpacking -> packing the packed Sequence will lead to exactly the same Object.\n",
    "inp, targets = next(iter(trainset_loader))\n",
    "\n",
    "batch_sizes = inp.batch_sizes\n",
    "inp2 = nn.utils.rnn.PackedSequence(inp.data, batch_sizes)\n",
    "assert torch.all(torch.eq(inp.data, inp2.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=89, hidden_size=256, num_classes=88) # 88 notes + 1 count is the input\n",
    "rnn = rnn.to(DEVICE)\n",
    "\n",
    "class_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.full((88,), POSITIVE_WEIGHT, device=DEVICE))\n",
    "poiss_criterion  = torch.nn.PoissonNLLLoss(log_input=True) # So this loss expect log(lambda) = x * b. Then it transforms it using exp.\n",
    "#So it expcects some linear function, that's why we can give the output of the neural network directly\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(rnn, class_criterion, poiss_criterion, loader, device):\n",
    "    rnn.eval()\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    losses_class = []\n",
    "    losses_poisson = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inp, target) in enumerate(loop):\n",
    "            inp, target = inp.to(device), target.to(device)\n",
    "            target = target.data\n",
    "        \n",
    "            target_notes, target_poiss = target[:,:-1], target[:, -1]\n",
    "            target_notes, target_poiss = target[:,:-1], target[:, -1]\n",
    "            logits, logits_poisson, _ = rnn(inp)\n",
    "\n",
    "            loss_class = class_criterion(logits, target_notes).item()\n",
    "            loss_poiss = poiss_criterion(logits_poisson, target_poiss).item()\n",
    "            \n",
    "            losses_class.append(loss_class)\n",
    "            losses_poisson.append(loss_poiss)\n",
    "            loop.set_postfix(loss_class=loss_class, loss_pois=loss_poiss)\n",
    "\n",
    "    rnn.train()\n",
    "    return sum(losses_class) / len(losses_class), sum(losses_poisson) / len(losses_poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, optimizer, class_criterion, poiss_criterion, loader, device, clip_value):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    losses_class = []\n",
    "    losses_poisson = []\n",
    "    \n",
    "    for idx, (inp, target) in enumerate(loop):\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "        target = target.data\n",
    "        \n",
    "        target_notes, target_poiss = target[:,:-1], target[:, -1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(): \n",
    "            logits, logits_poisson, _ = rnn(inp)\n",
    "            \n",
    "            loss_class = class_criterion(logits, target_notes)\n",
    "            loss_poiss = poiss_criterion(logits_poisson, target_poiss)\n",
    "            \n",
    "            loss = loss_class + loss_poiss # This in general may be negative number.\n",
    "             \n",
    "        scaler.scale(loss).backward()\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "        torch.nn.utils.clip_grad_norm_(rnn.parameters(), clip_value)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "        loss_class, loss_poiss = loss_class.item(), loss_poiss.item()\n",
    "        losses_class.append(loss_class)\n",
    "        losses_poisson.append(loss_poiss)\n",
    "        loop.set_postfix(loss_class=loss_class, loss_pois=loss_poiss)\n",
    "        \n",
    "    return sum(losses_class) / len(losses_class), sum(losses_poisson) / len(losses_poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:97: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "100%|██████████| 23/23 [00:40<00:00,  1.74s/it, loss_class=0.376, loss_pois=-.198]  \n",
      "100%|██████████| 6/6 [00:08<00:00,  1.38s/it, loss_class=0.342, loss_pois=-.895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "train_class_loss: 0.5994589885939723, train_poiss_loss: 0.25362121463631804\n",
      " val_class_loss: 0.34302642941474915, val_poiss_loss: -0.005009286105632782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:32<00:00,  1.40s/it, loss_class=0.158, loss_pois=0.254] \n",
      "100%|██████████| 6/6 [00:08<00:00,  1.36s/it, loss_class=0.159, loss_pois=-.76] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "train_class_loss: 0.20981809367304263, train_poiss_loss: 0.03953063066886819\n",
      " val_class_loss: 0.160321943461895, val_poiss_loss: -0.002836825946966807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:30<00:00,  1.31s/it, loss_class=0.15, loss_pois=-.213]   \n",
      "100%|██████████| 6/6 [00:09<00:00,  1.62s/it, loss_class=0.15, loss_pois=-.861] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "train_class_loss: 0.1563631859810456, train_poiss_loss: 0.04190560279201473\n",
      " val_class_loss: 0.15236622343460718, val_poiss_loss: -0.01256466656923294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:33<00:00,  1.47s/it, loss_class=0.152, loss_pois=0.116]  \n",
      "100%|██████████| 6/6 [00:07<00:00,  1.32s/it, loss_class=0.148, loss_pois=-.808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "train_class_loss: 0.15292014052038608, train_poiss_loss: 0.0338541719171664\n",
      " val_class_loss: 0.15171987563371658, val_poiss_loss: -0.011801789204279581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:39<00:00,  1.71s/it, loss_class=0.149, loss_pois=0.117] \n",
      "100%|██████████| 6/6 [00:12<00:00,  2.00s/it, loss_class=0.148, loss_pois=-.73] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "train_class_loss: 0.15188597077908722, train_poiss_loss: 0.0347357576146074\n",
      " val_class_loss: 0.15110349655151367, val_poiss_loss: 0.003673682610193888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip = 1.0\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "train_losses_class = []\n",
    "train_losses_poiss = []\n",
    "val_losses_class = []\n",
    "val_losses_poiss = []\n",
    "\n",
    "for epoch_number in range(NUM_EPOCHS):\n",
    "    train_class_loss, train_poiss_loss = train(rnn, optimizer, class_criterion, poiss_criterion, trainset_loader, DEVICE, CLIP_VALUE)    \n",
    "\n",
    "    train_losses_class.append(train_class_loss)\n",
    "    train_losses_poiss.append(train_poiss_loss)\n",
    "    \n",
    "    val_class_loss, val_poiss_loss = validate(rnn, class_criterion, poiss_criterion, valset_loader, DEVICE)\n",
    "\n",
    "    val_losses_class.append(val_class_loss)\n",
    "    val_losses_poiss.append(val_poiss_loss)\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch_number}:\\ntrain_class_loss: {train_class_loss}, train_poiss_loss: {train_poiss_loss}\\n val_class_loss: {val_class_loss}, val_poiss_loss: {val_poiss_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_piano_rnn(sample_length=4, temperature=1, starting_sequence=None):\n",
    "    #! This function is a little bit different, we must add one additional dimension to the sequence\n",
    "    #And also at each timestep we additionally predict counts using poisson distribution and returned parameter from neural net\n",
    "    if starting_sequence is None:\n",
    "        current_sequence_input = torch.zeros(1,1, 89, dtype=torch.float32, device=DEVICE)\n",
    "        current_sequence_input[0, 0, 45] = 1\n",
    "        current_sequence_input[0, 0, 48] = 1\n",
    "        current_sequence_input[0, 0, 52]= 1\n",
    "        current_sequence_input[0, 0, 88] = 1\n",
    "\n",
    "    final_output_sequence = [current_sequence_input.squeeze(1)]\n",
    "    \n",
    "    hidden = None\n",
    "    with torch.no_grad():\n",
    "        for i in range(sample_length):\n",
    "            current_sequence_input = torch.zeros(1,1, 89, dtype=torch.float32, device=DEVICE)\n",
    "            \n",
    "            logits_class, logits_poiss ,hidden = rnn(current_sequence_input, hidden)\n",
    "            probabilities = torch.sigmoid(logits_class.div(temperature)) # The less the temperature the bigger probabilities of 1 will be\n",
    "                          \n",
    "            \n",
    "            prob_of_0 = 1 - probabilities\n",
    "            dist = torch.stack((prob_of_0, probabilities), dim=3).squeeze() #Here we will get tensor [num_of_notes, 2]\n",
    "            \n",
    "            #from multinomial we have [num_of_notes, 1]. But eventually we want to have [1,1,num_of_notes]\n",
    "            current_sequence_input[0,0,:-1] = torch.multinomial(dist, 1).squeeze().unsqueeze(0).unsqueeze(1).to(torch.float32)\n",
    "            #print(current_sequence_input)\n",
    "            #break\n",
    "\n",
    "            lambda_ = np.exp(logits_poiss[0].item())\n",
    "            repetitions = max([1, np.random.poisson(lambda_,1)[0]])\n",
    "            #print(repetitions)\n",
    "            current_sequence_input[0,0,-1] = repetitions\n",
    "            final_output_sequence.append(current_sequence_input.squeeze(1))\n",
    "\n",
    "    sampled_sequence = torch.cat(final_output_sequence, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sampled_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_to_piano_roll(poiss_roll):\n",
    "    #Now we need to repeat each timestep as many times as it is predicted\n",
    "    notes, counts = poiss_roll[np.newaxis,:,:-1], poiss_roll[:,-1]\n",
    "    roll = np.repeat(notes[np.newaxis,0, 0], counts[0], axis=0)\n",
    "    \n",
    "    for timestep, count in zip(notes[0,1:,:], counts[1:]):\n",
    "        roll = np.concatenate((roll, np.repeat(timestep[np.newaxis,:], count, axis=0)), axis=0)\n",
    "        \n",
    "    return roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_from_piano_rnn(sample_length=200, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_roll = poisson_to_piano_roll(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(488, 88)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(sample[:,-1]))\n",
    "sample_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll = np.zeros((sample_roll.shape[0],128))\n",
    "roll[:, 21:109] = sample_roll\n",
    "roll[roll == 1] = 100\n",
    "track = pypianoroll.Multitrack(resolution=3)\n",
    "track.append(pypianoroll.StandardTrack(pianoroll=roll))\n",
    "pypianoroll.write(\"baseline2_song2.mid\", track)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a2214deb2e00a4588fb64d6e2ad9e78ab07788ce628f39696990503e7a4b014"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
