{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_BATCH_SIZE = 30\n",
    "VAL_BATCH_SIZE = 30\n",
    "DATA_PATH = '../data/Nottingham/'\n",
    "NUM_EPOCHS = 5\n",
    "POSITIVE_WEIGHT = 2\n",
    "CLIP_VALUE = 1.0 # clip value for the gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_pianoroll(path, poisson=True, resolution = 4):\n",
    "    #Resolution is set to 3 so that the sequences are not that long\n",
    "    midi_data = pypianoroll.read(path, resolution=resolution)\n",
    "    \n",
    "    piano_roll = midi_data.blend()[:, 21:109] #Taking just 81 usefull notes\n",
    "    \n",
    "    #we want to perform multilabel classification at each step so we need to binaryze the roll\n",
    "    piano_roll[piano_roll > 0] = 1\n",
    "    \n",
    "    if poisson:\n",
    "        current_roll = piano_roll[np.newaxis,0, :] # to have shape (1, num_of_notes)\n",
    "        count = 1\n",
    "        counts = []\n",
    "        new_piano_roll = current_roll\n",
    "        for i in range(1, piano_roll.shape[0]):\n",
    "            next_roll = piano_roll[np.newaxis, i, :]\n",
    "            if np.all(current_roll == next_roll):\n",
    "                count += 1\n",
    "            else:\n",
    "                counts.append(count)\n",
    "                count = 1\n",
    "                \n",
    "                new_piano_roll = np.concatenate((new_piano_roll, next_roll), axis=0)\n",
    "                \n",
    "                current_roll = next_roll\n",
    "                \n",
    "        counts.append(count)\n",
    "        new_piano_roll = np.concatenate((new_piano_roll, np.array(counts)[:,np.newaxis]), axis=1)\n",
    "        return new_piano_roll \n",
    "                           \n",
    "    return piano_roll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_path = os.path.join(DATA_PATH, \"train\", \"ashover_simple_chords_21.mid\")\n",
    "\n",
    "roll = path_to_pianoroll(midi_path, False,resolution = 8)\n",
    "\n",
    "roll2 = path_to_pianoroll(midi_path, True,resolution = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 88)\n",
      "(294, 89)\n",
      "[2 1 1 1 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2\n",
      " 2 1 7 1 2 1 1 1 2 1 7 1 2 1 1 1 2 1 5 2 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1 2 1\n",
      " 1 1 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2 2 1\n",
      " 7 1 2 1 1 1 2 1 7 1 2 1 1 1 2 1 5 2 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1 2 1 1 1\n",
      " 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2 2 1 7 1\n",
      " 2 1 1 1 2 1 7 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1 2 1 1\n",
      " 1 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 2 1 2 2 1 7\n",
      " 1 2 1 1 1 2 1 7 1 2 1 1 1 2 1 2 1 2 1 1 1 2 1 2 2 1 2 1 2 2 1 7 1 7 1]\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(roll.shape)\n",
    "print(roll2.shape) # This has less timesteps and the difference will be bigger w.r.t resolution, and this has one additional entry in 2ndim dim which are counts\n",
    "print(roll2[:,-1])\n",
    "print(np.sum(roll2[:,-1])) # Now the improvement is much bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    #Helper function for DataLoader\n",
    "    #Batch is a list of tuple in the form (input, target)\n",
    "    #We do not have to padd everything thanks to pack_sequence\n",
    "    data = [item[0] for item in batch] #\n",
    "    data = nn.utils.rnn.pack_sequence(data, enforce_sorted=False)\n",
    "    targets = [item[1] for item in batch]\n",
    "    targets = nn.utils.rnn.pack_sequence(targets, enforce_sorted=False)\n",
    "    return [data, targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesGenerationDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, path,):\n",
    "        \n",
    "        self.path = path\n",
    "        self.full_filenames = []\n",
    "        \n",
    "        #Here we assume that all midi files are valid, we do not check anything here.\n",
    "        for root, subdirs, files in os.walk(path):\n",
    "            for f in files:\n",
    "                self.full_filenames.append(os.path.join(root, f))\n",
    "                    \n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.full_filenames)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        full_filename = self.full_filenames[index]\n",
    "        \n",
    "        piano_roll = path_to_pianoroll(full_filename, poisson=True, resolution=8)\n",
    "        \n",
    "        #input and gt are shifted by one step w.r.t one another.\n",
    "        #we transpose it since piano_roll has shape [num_of_notes, number_of_event] we want to have format [number of events, num_of_notes]\n",
    "        input_sequence = piano_roll[:-1, :]\n",
    "        ground_truth_sequence = piano_roll[1:, :]\n",
    "        \n",
    "        return torch.tensor(input_sequence, dtype=torch.float32), torch.tensor(ground_truth_sequence, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NotesGenerationDataset(os.path.join(DATA_PATH, \"train\"))\n",
    "\n",
    "#ofc we want big batch_size. However, one training sample takes quite a lot of memory.\n",
    "#We will use torch.cuda.amp.autocast() so that we can make bigger batches\n",
    "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE,\n",
    "                                              shuffle=True, drop_last=True, collate_fn=collate)\n",
    "\n",
    "valset = NotesGenerationDataset(os.path.join(DATA_PATH, \"valid\"))\n",
    "\n",
    "valset_loader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small sanity check that our sets do not intersect at any moment\n",
    "train_songs = set(trainset.full_filenames)\n",
    "for song in valset.full_filenames:\n",
    "    assert not song in train_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n"
     ]
    }
   ],
   "source": [
    "print(trainset.__len__())\n",
    "assert len(os.listdir(os.path.join(DATA_PATH, \"train\"))) == trainset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size # amount of different notes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #At first we need layer that will encode our vector with only once to better representation\n",
    "        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        #At the end we want to get vector with logits of all notes\n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.poisson_fc = nn.Linear(hidden_size, 1) # I know that I could merge these into one layer, but this is more readable\n",
    "        #If this solution will work well for the small dataset then I'll try to optimize it more \n",
    "    \n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        \n",
    "        if isinstance(inp, nn.utils.rnn.PackedSequence):\n",
    "            #If we have Packed sequence we proceed a little bit differently\n",
    "            batch_sizes = inp.batch_sizes\n",
    "            notes_encoded = self.notes_encoder(inp.data) #PackedSequence.data is a tensor representation of shape [samples, num_of_notes]\n",
    "            rnn_in = nn.utils.rnn.PackedSequence(notes_encoded,batch_sizes) #This is not recommended in PyTorch documentation.\n",
    "            #However this saves a day here. Since otherwise we would have to create padded sequences \n",
    "            outputs, hidden = self.lstm(rnn_in, hidden)\n",
    "            \n",
    "            class_logits = self.logits_fc(outputs.data) #Again we go from packedSequence to tensor.\n",
    "            poisson_logits = self.poisson_fc(outputs.data)\n",
    "            \n",
    "        else:\n",
    "            #If we have tensor at the input this is pretty straightforward\n",
    "            notes_encoded = self.notes_encoder(inp)\n",
    "            outputs, hidden = self.lstm(notes_encoded, hidden)\n",
    "            class_logits = self.logits_fc(outputs)\n",
    "            poisson_logits = self.poisson_fc(outputs)\n",
    "        \n",
    "        return class_logits, poisson_logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:97: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Now sanity check about Packed Sequences. So I check if Unpacking -> packing the packed Sequence will lead to exactly the same Object.\n",
    "inp, targets = next(iter(trainset_loader))\n",
    "\n",
    "batch_sizes = inp.batch_sizes\n",
    "inp2 = nn.utils.rnn.PackedSequence(inp.data, batch_sizes)\n",
    "assert torch.all(torch.eq(inp.data, inp2.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=89, hidden_size=256, num_classes=88) # 88 notes + 1 count is the input\n",
    "rnn = rnn.to(DEVICE)\n",
    "\n",
    "class_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.full((88,), POSITIVE_WEIGHT, device=DEVICE))\n",
    "poiss_criterion  = torch.nn.PoissonNLLLoss(log_input=True) # So this loss expect log(lambda) = x * b. Then it transforms it using exp.\n",
    "#So it expcects some linear function, that's why we can give the output of the neural network directly\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(rnn, class_criterion, poiss_criterion, loader, device):\n",
    "    rnn.eval()\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    losses_class = []\n",
    "    losses_poisson = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inp, target) in enumerate(loop):\n",
    "            inp, target = inp.to(device), target.to(device)\n",
    "            target = target.data\n",
    "        \n",
    "            target_notes, target_poiss = target[:,:-1], target[:, -1]\n",
    "            target_notes, target_poiss = target[:,:-1], target[:, -1]\n",
    "            logits, logits_poisson, _ = rnn(inp)\n",
    "\n",
    "            loss_class = class_criterion(logits, target_notes).item()\n",
    "            loss_poiss = poiss_criterion(logits_poisson, target_poiss).item()\n",
    "            \n",
    "            losses_class.append(loss_class)\n",
    "            losses_poisson.append(loss_poiss)\n",
    "            loop.set_postfix(loss_class=loss_class, loss_pois=loss_poiss)\n",
    "\n",
    "    rnn.train()\n",
    "    return sum(losses_class) / len(losses_class), sum(losses_poisson) / len(losses_poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, optimizer, class_criterion, poiss_criterion, loader, device, clip_value):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    losses_class = []\n",
    "    losses_poisson = []\n",
    "    \n",
    "    for idx, (inp, target) in enumerate(loop):\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "        target = target.data\n",
    "        #print(target.shape)\n",
    "        target_notes, target_poiss = target[:,:-1], target[:, -1]\n",
    "        #print(target_notes)\n",
    "        #print(target_poiss)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(): \n",
    "            logits, logits_poisson, _ = rnn(inp)\n",
    "            \n",
    "            loss_class = class_criterion(logits, target_notes)\n",
    "            loss_poiss = poiss_criterion(logits_poisson, target_poiss)\n",
    "            \n",
    "            loss = loss_class + loss_poiss # This in general may be negative number.\n",
    "             \n",
    "        scaler.scale(loss).backward()\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "        torch.nn.utils.clip_grad_norm_(rnn.parameters(), clip_value)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "        loss_class, loss_poiss = loss_class.item(), loss_poiss.item()\n",
    "        losses_class.append(loss_class)\n",
    "        losses_poisson.append(loss_poiss)\n",
    "        loop.set_postfix(loss_class=loss_class, loss_pois=loss_poiss)\n",
    "        \n",
    "    return sum(losses_class) / len(losses_class), sum(losses_poisson) / len(losses_poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:18<00:00,  1.24it/s, loss_class=0.4, loss_pois=0.106]    \n",
      "100%|██████████| 6/6 [00:04<00:00,  1.46it/s, loss_class=0.358, loss_pois=-.86] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "train_class_loss: 0.6035348008508268, train_poiss_loss: 0.2711894668476737\n",
      " val_class_loss: 0.35883162915706635, val_poiss_loss: -0.012340746819972992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:18<00:00,  1.22it/s, loss_class=0.167, loss_pois=0.0645] \n",
      "100%|██████████| 6/6 [00:04<00:00,  1.47it/s, loss_class=0.156, loss_pois=-.823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "train_class_loss: 0.20855830415435458, train_poiss_loss: 0.03980000206755231\n",
      " val_class_loss: 0.15972882757584253, val_poiss_loss: -0.012017610172430674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:18<00:00,  1.23it/s, loss_class=0.154, loss_pois=0.158] \n",
      "100%|██████████| 6/6 [00:03<00:00,  1.52it/s, loss_class=0.15, loss_pois=-.733] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "train_class_loss: 0.15601511558760767, train_poiss_loss: 0.03707794506993631\n",
      " val_class_loss: 0.15209558109442392, val_poiss_loss: 0.003101050853729248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:18<00:00,  1.23it/s, loss_class=0.158, loss_pois=0.0703]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.45it/s, loss_class=0.149, loss_pois=-.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "train_class_loss: 0.15251916258231454, train_poiss_loss: 0.04380273640803669\n",
      " val_class_loss: 0.15106557806332907, val_poiss_loss: -0.009540058672428131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5/23 [00:05<00:19,  1.08s/it, loss_class=0.153, loss_pois=0.0543]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13280/397407566.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch_number\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_class_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_poiss_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoiss_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainset_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP_VALUE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtrain_losses_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_class_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13280/1156775934.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(rnn, optimizer, class_criterion, poiss_criterion, loader, device, clip_value)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"found_inf_per_device\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"stage\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"found_inf_per_device\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m             \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"found_inf_per_device\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m             \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clip = 1.0\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "train_losses_class = []\n",
    "train_losses_poiss = []\n",
    "val_losses_class = []\n",
    "val_losses_poiss = []\n",
    "\n",
    "for epoch_number in range(NUM_EPOCHS):\n",
    "    train_class_loss, train_poiss_loss = train(rnn, optimizer, class_criterion, poiss_criterion, trainset_loader, DEVICE, CLIP_VALUE)    \n",
    "\n",
    "    train_losses_class.append(train_class_loss)\n",
    "    train_losses_poiss.append(train_poiss_loss)\n",
    "    \n",
    "    val_class_loss, val_poiss_loss = validate(rnn, class_criterion, poiss_criterion, valset_loader, DEVICE)\n",
    "\n",
    "    val_losses_class.append(val_class_loss)\n",
    "    val_losses_poiss.append(val_poiss_loss)\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch_number}:\\ntrain_class_loss: {train_class_loss}, train_poiss_loss: {train_poiss_loss}\\n val_class_loss: {val_class_loss}, val_poiss_loss: {val_poiss_loss}\")\n",
    "    # if current_val_loss < best_val_loss:\n",
    "        \n",
    "    #     torch.save(rnn.state_dict(), 'music_rnn.pth')\n",
    "    #     best_val_loss = current_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_piano_rnn(sample_length=4, temperature=1, starting_sequence=None, deterministic = False, threshold=0.5):\n",
    "\n",
    "    if starting_sequence is None:\n",
    "        current_sequence_input = torch.zeros(1,1, 89, dtype=torch.float32, device=DEVICE)\n",
    "        current_sequence_input[0, 0, 40] = 1\n",
    "        current_sequence_input[0, 0, 50] = 1\n",
    "        current_sequence_input[0, 0, 56] = 1\n",
    "        current_sequence_input[0, 0, 88] = 1\n",
    "\n",
    "    final_output_sequence = [current_sequence_input.squeeze(1)]\n",
    "    \n",
    "    hidden = None\n",
    "    with torch.no_grad():\n",
    "        for i in range(sample_length):\n",
    "            current_sequence_input = torch.zeros(1,1, 89, dtype=torch.float32, device=DEVICE)\n",
    "            \n",
    "            logits_class, logits_poiss ,hidden = rnn(current_sequence_input, hidden)\n",
    "            probabilities = torch.sigmoid(logits_class.div(temperature)) # The less the temperature the bigger probabilities of 1 will be\n",
    "            if deterministic and len(final_output_sequence) > 5:\n",
    "                current_sequence_input[0,0,:-1] = (probabilities > threshold).to(torch.float32)                \n",
    "            else:\n",
    "                prob_of_0 = 1 - probabilities\n",
    "                dist = torch.stack((prob_of_0, probabilities), dim=3).squeeze() #Here we will get tensor [num_of_notes, 2]\n",
    "                \n",
    "                #from multinomial we have [num_of_notes, 1]. But eventually we want to have [1,1,num_of_notes]\n",
    "                current_sequence_input[0,0,:-1] = torch.multinomial(dist, 1).squeeze().unsqueeze(0).unsqueeze(1).to(torch.float32)\n",
    "                #print(current_sequence_input)\n",
    "                #break\n",
    "\n",
    "            lambda_ = np.exp(logits_poiss[0].item())\n",
    "            repetitions = max([1, np.random.poisson(lambda_,1)[0]])\n",
    "            #print(repetitions)\n",
    "            current_sequence_input[0,0,-1] = repetitions\n",
    "            final_output_sequence.append(current_sequence_input.squeeze(1))\n",
    "\n",
    "    sampled_sequence = torch.cat(final_output_sequence, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sampled_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_to_piano_roll(poiss_roll):\n",
    "    notes, counts = poiss_roll[np.newaxis,:,:-1], poiss_roll[:,-1]\n",
    "    roll = np.repeat(notes[np.newaxis,0, 0], counts[0], axis=0)\n",
    "    \n",
    "    for timestep, count in zip(notes[0,1:,:], counts[1:]):\n",
    "        roll = np.concatenate((roll, np.repeat(timestep[np.newaxis,:], count, axis=0)), axis=0)\n",
    "        \n",
    "    return roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_from_piano_rnn(sample_length=200, temperature=0.5, deterministic=False, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_roll = poisson_to_piano_roll(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(531, 88)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(sample[:,-1]))\n",
    "sample_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll = np.zeros((sample_roll.shape[0],128))\n",
    "roll[:, 21:109] = sample_roll\n",
    "roll[roll == 1] = 100\n",
    "track = pypianoroll.Multitrack(resolution=3)\n",
    "track.append(pypianoroll.StandardTrack(pianoroll=roll))\n",
    "pypianoroll.write(\"sample2.mid\", track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a2214deb2e00a4588fb64d6e2ad9e78ab07788ce628f39696990503e7a4b014"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
