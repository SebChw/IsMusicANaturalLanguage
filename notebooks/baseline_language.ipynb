{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach I'm using completely different representation of Midi song. Here working directly with pianorolls is time consuming. So I've prepared some txt files with the language. This represenation is cool because it has very small dictonary size compared with previous representation. So now we can use typical NLP approach and predict next words given sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "from midiToTxt import converter2 # This needed to convert our textual representation back to midi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_folder_to_txt(midi_folder, destination, song_separator=\"\\n\"):\n",
    "    \"\"\"This function will prepare folder with training data for you.\n",
    "\n",
    "    Args:\n",
    "        midi_folder (str): folder with midi files\n",
    "        destination (str): folder where txt will be saved\n",
    "        song_separator (str, optional): Special token that will separate songs within one txt file. Defaults to \"\\n\".\n",
    "    \"\"\"\n",
    "    converter = converter2.BetterMidiToTxtConverter()\n",
    "    with open(destination,'w') as dest_file:\n",
    "        for root, subdirs, files in os.walk(midi_folder):\n",
    "            for f in files:\n",
    "                final_path = os.path.join(root,f)\n",
    "                \n",
    "                dest_file.write(converter.midi_to_str(final_path))\n",
    "                dest_file.write(song_separator)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = \"../data\"\n",
    "MIDI_FOLDER = os.path.join(BASE_FOLDER, \"Nottingham\")\n",
    "TXT_FOLDER = os.path.join(BASE_FOLDER, \"Nottingham_txt\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_BATCH_SIZE = 30\n",
    "VAL_BATCH_SIZE = 30\n",
    "NUM_EPOCHS =10\n",
    "POSITIVE_WEIGHT = 1\n",
    "CLIP_VALUE = 1.0 # clip value for the gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:97: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for subfolder in os.listdir(MIDI_FOLDER):\n",
    "    midi_folder_to_txt(os.path.join(MIDI_FOLDER, subfolder), os.path.join(TXT_FOLDER, subfolder + \".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This time we need special vocabulary class that can perform word -> number tokens conversion and vice versa\n",
    "#Each number represents one word in that dictonary\n",
    "#With small datasets we can create vocab from scratch every time. But if you need to work with big corpuses please create some json with vocab\n",
    "class Vocabulary:\n",
    "    def __init__(self, folder_with_txt, song_separator=\"\\n\"):\n",
    "        self.folder_with_txt = folder_with_txt\n",
    "        self.song_separator = song_separator\n",
    "        self.build_mappings()\n",
    "        \n",
    "    def build_mappings(self):\n",
    "        corpus = \"\"\n",
    "        for root, subdirs, files in os.walk(self.folder_with_txt):\n",
    "            for f in files:\n",
    "                final_path = os.path.join(root,f)\n",
    "                with open(final_path, 'r') as f:\n",
    "                    corpus = \"\".join([corpus, f.read().replace(self.song_separator, \" \")])\n",
    "                    \n",
    "        self.unique_words = set(corpus.strip().split(\" \"))\n",
    "        self.vocab_length = len(self.unique_words)\n",
    "        #we need int to word and word to int conversion\n",
    "        self.int_to_word = {index : word for index,word in enumerate(self.unique_words)}\n",
    "        self.word_to_int = {word: index for index,word in self.int_to_word.items()}\n",
    "        \n",
    "    def tokenize_song(self, song):\n",
    "        return song.strip().split(\" \")\n",
    "    \n",
    "    def numberalize_song(self, song):\n",
    "        numberalized = []\n",
    "        \n",
    "        for token in self.tokenize_song(song):\n",
    "            numberalized.append(self.word_to_int[token])\n",
    "            \n",
    "        return numberalized\n",
    "            \n",
    "    def numberlized_to_text(self, numberalized):\n",
    "        song = []\n",
    "        \n",
    "        for token in numberalized:\n",
    "            song.append(self.int_to_word[token])\n",
    "            \n",
    "        return \" \".join(song)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesGenerationDataset(data.Dataset):\n",
    "    def __init__(self, path, vocab, song_separator = \"\\n\"):\n",
    "        self.path = path\n",
    "        self.vocab = vocab\n",
    "        self.numberalized_songs = []\n",
    "        self.song_separator = song_separator\n",
    "        \n",
    "        #Here we have all the songs loaded into memory, to make training faster\n",
    "        with open(path, \"r\") as f:\n",
    "            text = f.read().strip()\n",
    "            for song in text.split(song_separator):\n",
    "                self.numberalized_songs.append(vocab.numberalize_song(song))\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.numberalized_songs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        numberalized_song = self.numberalized_songs[index]\n",
    "        #We don't return one hot encoded vectors here since PyTorch has cool functonality for word embedings.\n",
    "        #This is quite different situation from these in previous experiment, where we do not have one-hot-vector but just a vector of 1's and 0's\n",
    "        return torch.tensor(numberalized_song[:-1], dtype=torch.int), torch.tensor(numberalized_song[1:], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    #Helper function for DataLoader\n",
    "    #Batch is a list of tuple in the form (input, target)\n",
    "    #We do not have to padd everything thanks to pack_sequence\n",
    "    data = [item[0] for item in batch] #\n",
    "    data = nn.utils.rnn.pack_sequence(data, enforce_sorted=False)\n",
    "    targets = [item[1] for item in batch]\n",
    "    targets = nn.utils.rnn.pack_sequence(targets, enforce_sorted=False)\n",
    "    return [data, targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(TXT_FOLDER)\n",
    "\n",
    "trainset = NotesGenerationDataset(os.path.join(TXT_FOLDER, \"train.txt\"),vocab)\n",
    "\n",
    "#ofc we want big batch_size. However, one training sample takes quite a lot of memory.\n",
    "#We will use torch.cuda.amp.autocast() so that we can make bigger batches\n",
    "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE,\n",
    "                                              shuffle=True, drop_last=True, collate_fn=collate)\n",
    "\n",
    "valset = NotesGenerationDataset(os.path.join(TXT_FOLDER, \"valid.txt\"),vocab)\n",
    "\n",
    "valset_loader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([689]), torch.Size([689]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainset.__len__())\n",
    "trainset.__getitem__(0)[0].shape, trainset.__getitem__(0)[1].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #nn.Embeding does the same job as nn.Linear but works like a lookuptable\n",
    "        self.notes_encoder = nn.Embedding(num_embeddings=num_classes, embedding_dim=hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        #At the end we want to get vector with logits of all notes\n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        \n",
    "        if isinstance(inp, nn.utils.rnn.PackedSequence):\n",
    "            #If we have Packed sequence we proceed a little bit differently\n",
    "            batch_sizes = inp.batch_sizes\n",
    "            \n",
    "            notes_encoded = self.notes_encoder(inp.data) #PackedSequence.data is a tensor representation of shape [samples, num_of_notes]\n",
    "            \n",
    "            rnn_in = nn.utils.rnn.PackedSequence(notes_encoded,batch_sizes) #This is not recommended in PyTorch documentation.\n",
    "            #However this saves a day here. Since otherwise we would have to create padded sequences \n",
    "            outputs, hidden = self.lstm(rnn_in, hidden)\n",
    "            \n",
    "            logits = self.logits_fc(outputs.data) #Again we go from packedSequence to tensor.\n",
    "        else:\n",
    "            #If we have tensor at the input this is pretty straightforward\n",
    "            notes_encoded = self.notes_encoder(inp)\n",
    "            outputs, hidden = self.lstm(notes_encoded, hidden)\n",
    "            logits = self.logits_fc(outputs)\n",
    "            \n",
    "        \n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(hidden_size=256, num_classes=vocab.vocab_length)\n",
    "rnn = rnn.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! sanity check of the network\n",
    "# inp, targets = next(iter(trainset_loader))\n",
    "# logits, _ =rnn.forward(inp.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(rnn, criterion, loader, device):\n",
    "    rnn.eval()\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inp, target) in enumerate(loop):\n",
    "            inp, target = inp.to(device), target.to(device)\n",
    "            logits, _ = rnn(inp)\n",
    "\n",
    "            loss = criterion(logits, target.data).item()\n",
    "            \n",
    "            losses.append(loss)\n",
    "            \n",
    "            loop.set_postfix(loss = loss)\n",
    "\n",
    "    rnn.train()\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, optimizer, criterion, loader, device, clip_value):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for idx, (inp, target) in enumerate(loop):\n",
    "        inp, target = inp.to(device), target.to(device) # remember that target is packed sequence!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(): \n",
    "            logits, _ = rnn(inp)\n",
    "            \n",
    "            loss = criterion(logits, target.data)\n",
    "             \n",
    "        scaler.scale(loss).backward()\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "        torch.nn.utils.clip_grad_norm_(rnn.parameters(), clip_value)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "        loss = loss.item()\n",
    "        losses.append(loss)\n",
    "        loop.set_postfix(loss=loss)\n",
    "        \n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:08<00:00,  2.77it/s, loss=2.74]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.85it/s, loss=2.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "train_loss: 3.3261376774829365\n",
      " val_loss: 2.660994013150533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:12<00:00,  1.83it/s, loss=1.85]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.90it/s, loss=2.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "train_loss: 2.2536923418874326\n",
      " val_loss: 1.816677192846934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:12<00:00,  1.85it/s, loss=1.39]\n",
      "100%|██████████| 6/6 [00:01<00:00,  5.05it/s, loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "train_loss: 1.5973703964896824\n",
      " val_loss: 1.3867463668187459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:08<00:00,  2.70it/s, loss=1.18]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.13it/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "train_loss: 1.3213616972384246\n",
      " val_loss: 1.217357873916626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:08<00:00,  2.78it/s, loss=1.11]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.75it/s, loss=1.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "train_loss: 1.1851357791734778\n",
      " val_loss: 1.108250101407369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:13<00:00,  1.76it/s, loss=1.02] \n",
      "100%|██████████| 6/6 [00:01<00:00,  4.72it/s, loss=1.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "train_loss: 1.0798903651859448\n",
      " val_loss: 1.016643613576889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:12<00:00,  1.78it/s, loss=1.04] \n",
      "100%|██████████| 6/6 [00:01<00:00,  4.60it/s, loss=0.997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "train_loss: 1.0042100237763447\n",
      " val_loss: 0.9496674736340841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:12<00:00,  1.78it/s, loss=1]    \n",
      "100%|██████████| 6/6 [00:01<00:00,  4.53it/s, loss=0.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "train_loss: 0.9426117202510005\n",
      " val_loss: 0.9049824972947439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:13<00:00,  1.76it/s, loss=0.886]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.42it/s, loss=0.891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "train_loss: 0.8968298849852189\n",
      " val_loss: 0.8611185153325399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:13<00:00,  1.74it/s, loss=0.839]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.34it/s, loss=0.856]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "train_loss: 0.8605392393858536\n",
      " val_loss: 0.8313474853833517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip = 1.0\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch_number in range(NUM_EPOCHS):\n",
    "    train_loss = train(rnn, optimizer, criterion, trainset_loader, DEVICE, CLIP_VALUE)    \n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    val_loss = validate(rnn, criterion, valset_loader, DEVICE)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch_number}:\\ntrain_loss: {train_loss}\\n val_loss: {val_loss}\")\n",
    "    # if current_val_loss < best_val_loss:\n",
    "        \n",
    "    #     torch.save(rnn.state_dict(), 'music_rnn.pth')\n",
    "    #     best_val_loss = current_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_piano_rnn(rnn, vocab : Vocabulary, sample_length=4, temperature=1, starting_sequence=None):\n",
    "    #Here we're not directly building piano roll but that textual representation of the song\n",
    "    if starting_sequence is None:\n",
    "        current_sequence_input = torch.tensor([vocab.word_to_int[\"n72\"]], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    final_output_sequence = [current_sequence_input.item()]\n",
    "    \n",
    "    hidden = None\n",
    "    with torch.no_grad():\n",
    "        for i in range(sample_length):\n",
    "            logits ,hidden = rnn(current_sequence_input.to(DEVICE), hidden)\n",
    "            logits = logits.squeeze(0)\n",
    "            probabilities = torch.softmax(logits.div(temperature), dim=1) # The less the temperature the bigger probabilities of 1 will be\n",
    "            #from multinomial we have [num_of_notes, 1]. But eventually we want to have [1,1,num_of_notes]\n",
    "            selected = torch.multinomial(probabilities, 1)\n",
    "            current_sequence_input = selected\n",
    "        \n",
    "            final_output_sequence.append(selected.item())\n",
    "\n",
    "    return final_output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_from_piano_rnn(rnn,vocab,sample_length=201, temperature=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = vocab.numberlized_to_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n72 t3 w t4 n69 t3 w t4 n55 t31 n59 t31 n62 t31 n67 t7 w t8 n79 t7 w t8 n71 t7 w t8 n69 t7 w t8 n55 t31 n59 t31 n62 t31 n71 t7 w t8 n67 t7 w t8 n69 t7 w t8 n67 t7 w t8 n55 t31 n59 t31 n62 t31 n71 t7 w t8 n71 t7 w t8 n71 t7 w t8 n69 t7 w t8 n55 t31 n59 t31 n62 t31 n71 t7 w t8 n67 t7 w t8 n79 t7 w t8 n74 t7 w t8 n55 t31 n59 t31 n62 t31 n67 t7 w t8 n67 t7 w t8 n67 t7 w t8 n67 t7 w t8 n50 t31 n54 t31 n57 t31 n74 t7 w t8 n74 t7 w t8 n74 t7 w t8 n74 t7 w t8 n55 t31 n59 t31 n62 t31 n71 t7 w t8 n71 t7 w t8 n74 t7 w t8 n67 t7 w t8 n55 t31 n59 t31 n62 t31 n67 t7 w t8 n71 t7 w t8 n76 t7 w t8 n67 t7 w t8 n55 t31 n59 t31 n62 t31 n74 t7 w t8 n69 t7 w t8 n74 t7 w t8'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = converter2.BetterMidiToTxtConverter()\n",
    "converter.set_biggest_roll((200,128))\n",
    "converter.str_to_midi(song, \"baseline3.mid\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a2214deb2e00a4588fb64d6e2ad9e78ab07788ce628f39696990503e7a4b014"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
