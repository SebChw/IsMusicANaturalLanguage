{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pypianoroll\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "from midiToTxt import converter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_folder_to_txt(midi_folder, destination, song_separator=\"\\n\"):\n",
    "    converter = converter2.BetterMidiToTxtConverter()\n",
    "    with open(destination,'w') as dest_file:\n",
    "        for root, subdirs, files in os.walk(midi_folder):\n",
    "            for f in files:\n",
    "                final_path = os.path.join(root,f)\n",
    "                \n",
    "                dest_file.write(converter.midi_to_str(final_path))\n",
    "                dest_file.write(song_separator)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = \"../data\"\n",
    "MIDI_FOLDER = os.path.join(BASE_FOLDER, \"Nottingham\")\n",
    "TXT_FOLDER = os.path.join(BASE_FOLDER, \"Nottingham_txt\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_BATCH_SIZE = 30\n",
    "VAL_BATCH_SIZE = 30\n",
    "NUM_EPOCHS =10\n",
    "POSITIVE_WEIGHT = 1\n",
    "CLIP_VALUE = 1.0 # clip value for the gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:97: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for subfolder in os.listdir(MIDI_FOLDER):\n",
    "    midi_folder_to_txt(os.path.join(MIDI_FOLDER, subfolder), os.path.join(TXT_FOLDER, subfolder + \".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, folder_with_txt, song_separator=\"\\n\"):\n",
    "        self.folder_with_txt = folder_with_txt\n",
    "        self.song_separator = song_separator\n",
    "        self.build_mappings()\n",
    "        \n",
    "    def build_mappings(self):\n",
    "        corpus = \"\"\n",
    "        for root, subdirs, files in os.walk(self.folder_with_txt):\n",
    "            for f in files:\n",
    "                final_path = os.path.join(root,f)\n",
    "                with open(final_path, 'r') as f:\n",
    "                    corpus = \"\".join([corpus, f.read().replace(self.song_separator, \" \")])\n",
    "                    \n",
    "        self.unique_words = set(corpus.strip().split(\" \"))\n",
    "        self.vocab_length = len(self.unique_words)\n",
    "        self.int_to_word = {index : word for index,word in enumerate(self.unique_words)}\n",
    "        self.word_to_int = {word: index for index,word in self.int_to_word.items()}\n",
    "        \n",
    "    def tokenize_song(self, song):\n",
    "        return song.strip().split(\" \")\n",
    "    \n",
    "    def numberalize_song(self, song):\n",
    "        numberalized = []\n",
    "        \n",
    "        for token in self.tokenize_song(song):\n",
    "            numberalized.append(self.word_to_int[token])\n",
    "            \n",
    "        return numberalized\n",
    "            \n",
    "    def numberlized_to_text(self, numberalized):\n",
    "        song = []\n",
    "        \n",
    "        for token in numberalized:\n",
    "            song.append(self.int_to_word[token])\n",
    "            \n",
    "        return \" \".join(song)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesGenerationDataset(data.Dataset):\n",
    "    def __init__(self, path, vocab, song_separator = \"\\n\"):\n",
    "        self.path = path\n",
    "        self.vocab = vocab\n",
    "        self.numberalized_songs = []\n",
    "        self.song_separator = song_separator\n",
    "        \n",
    "        with open(path, \"r\") as f:\n",
    "            text = f.read().strip()\n",
    "            for song in text.split(song_separator):\n",
    "                self.numberalized_songs.append(vocab.numberalize_song(song))\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.numberalized_songs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        numberalized_song = self.numberalized_songs[index]\n",
    "        #We don't return one hot encoded vectors here since PyTorch has cool functonality for word embedings.\n",
    "        #This is quite different situation from these in previous experiment, where we do not have one-hot-vector but just a vector of 1's and 0's\n",
    "        return torch.tensor(numberalized_song[:-1], dtype=torch.int), torch.tensor(numberalized_song[1:], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    #Helper function for DataLoader\n",
    "    #Batch is a list of tuple in the form (input, target)\n",
    "    #We do not have to padd everything thanks to pack_sequence\n",
    "    data = [item[0] for item in batch] #\n",
    "    data = nn.utils.rnn.pack_sequence(data, enforce_sorted=False)\n",
    "    targets = [item[1] for item in batch]\n",
    "    targets = nn.utils.rnn.pack_sequence(targets, enforce_sorted=False)\n",
    "    return [data, targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(TXT_FOLDER)\n",
    "\n",
    "trainset = NotesGenerationDataset(os.path.join(TXT_FOLDER, \"train.txt\"),vocab)\n",
    "\n",
    "#ofc we want big batch_size. However, one training sample takes quite a lot of memory.\n",
    "#We will use torch.cuda.amp.autocast() so that we can make bigger batches\n",
    "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE,\n",
    "                                              shuffle=True, drop_last=True, collate_fn=collate)\n",
    "\n",
    "valset = NotesGenerationDataset(os.path.join(TXT_FOLDER, \"valid.txt\"),vocab)\n",
    "\n",
    "valset_loader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([689]), torch.Size([689]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainset.__len__())\n",
    "trainset.__getitem__(0)[0].shape, trainset.__getitem__(0)[1].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #nn.Embeding does the same job as nn.Linear but works like a lookuptable\n",
    "        self.notes_encoder = nn.Embedding(num_embeddings=num_classes, embedding_dim=hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        #At the end we want to get vector with logits of all notes\n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        \n",
    "        if isinstance(inp, nn.utils.rnn.PackedSequence):\n",
    "            #If we have Packed sequence we proceed a little bit differently\n",
    "            batch_sizes = inp.batch_sizes\n",
    "            #print(inp.data.shape)\n",
    "            notes_encoded = self.notes_encoder(inp.data) #PackedSequence.data is a tensor representation of shape [samples, num_of_notes]\n",
    "            #print(notes_encoded.shape)\n",
    "            rnn_in = nn.utils.rnn.PackedSequence(notes_encoded,batch_sizes) #This is not recommended in PyTorch documentation.\n",
    "            #However this saves a day here. Since otherwise we would have to create padded sequences \n",
    "            outputs, hidden = self.lstm(rnn_in, hidden)\n",
    "            #print(outputs.data.shape)\n",
    "            \n",
    "            logits = self.logits_fc(outputs.data) #Again we go from packedSequence to tensor.\n",
    "            #print(logits.shape)\n",
    "            \n",
    "        else:\n",
    "            #If we have tensor at the input this is pretty straightforward\n",
    "            notes_encoded = self.notes_encoder(inp)\n",
    "            outputs, hidden = self.lstm(notes_encoded, hidden)\n",
    "            logits = self.logits_fc(outputs)\n",
    "            \n",
    "        \n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(hidden_size=256, num_classes=vocab.vocab_length)\n",
    "rnn = rnn.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! sanity check of the network\n",
    "# inp, targets = next(iter(trainset_loader))\n",
    "# logits, _ =rnn.forward(inp.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(rnn, criterion, loader, device):\n",
    "    rnn.eval()\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inp, target) in enumerate(loop):\n",
    "            inp, target = inp.to(device), target.to(device)\n",
    "            logits, _ = rnn(inp)\n",
    "\n",
    "            loss = criterion(logits, target.data).item()\n",
    "            \n",
    "            losses.append(loss)\n",
    "            \n",
    "            loop.set_postfix(loss = loss)\n",
    "\n",
    "    rnn.train()\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, optimizer, criterion, loader, device, clip_value):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for idx, (inp, target) in enumerate(loop):\n",
    "        inp, target = inp.to(device), target.to(device) # remember that target is packed sequence!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(): \n",
    "            logits, _ = rnn(inp)\n",
    "            \n",
    "            loss = criterion(logits, target.data)\n",
    "             \n",
    "        scaler.scale(loss).backward()\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "        torch.nn.utils.clip_grad_norm_(rnn.parameters(), clip_value)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "        loss = loss.item()\n",
    "        losses.append(loss)\n",
    "        loop.set_postfix(loss=loss)\n",
    "        \n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:06<00:00,  3.30it/s, loss=1.05]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.50it/s, loss=1.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "train_loss: 1.1172740874083147\n",
      " val_loss: 1.0471152861913045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:07<00:00,  3.26it/s, loss=1.03] \n",
      "100%|██████████| 6/6 [00:00<00:00,  6.51it/s, loss=1.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "train_loss: 1.0309797525405884\n",
      " val_loss: 0.9718188246091207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:06<00:00,  3.35it/s, loss=0.934]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.51it/s, loss=0.952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "train_loss: 0.9602655872054722\n",
      " val_loss: 0.9144785006841024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:06<00:00,  3.32it/s, loss=0.874]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.53it/s, loss=0.915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "train_loss: 0.9089342226152834\n",
      " val_loss: 0.8779982030391693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:06<00:00,  3.31it/s, loss=0.831]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.48it/s, loss=0.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "train_loss: 0.8727929825368135\n",
      " val_loss: 0.8451782564322153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:06<00:00,  3.33it/s, loss=0.856]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.50it/s, loss=0.835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "train_loss: 0.8462154943010082\n",
      " val_loss: 0.8104888498783112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:06<00:00,  3.36it/s, loss=0.804]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.51it/s, loss=0.808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "train_loss: 0.8084866663684016\n",
      " val_loss: 0.7785990635553995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:06<00:00,  3.32it/s, loss=0.758]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.40it/s, loss=0.787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "train_loss: 0.7796819546948308\n",
      " val_loss: 0.7589213252067566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:06<00:00,  3.36it/s, loss=0.713]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.46it/s, loss=0.757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "train_loss: 0.7556408203166464\n",
      " val_loss: 0.7353298862775167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:06<00:00,  3.31it/s, loss=0.736]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.47it/s, loss=0.734]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "train_loss: 0.7354539036750793\n",
      " val_loss: 0.7118591070175171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip = 1.0\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch_number in range(NUM_EPOCHS):\n",
    "    train_loss = train(rnn, optimizer, criterion, trainset_loader, DEVICE, CLIP_VALUE)    \n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    val_loss = validate(rnn, criterion, valset_loader, DEVICE)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch_number}:\\ntrain_loss: {train_loss}\\n val_loss: {val_loss}\")\n",
    "    # if current_val_loss < best_val_loss:\n",
    "        \n",
    "    #     torch.save(rnn.state_dict(), 'music_rnn.pth')\n",
    "    #     best_val_loss = current_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_piano_rnn(rnn, vocab : Vocabulary, sample_length=4, temperature=1, starting_sequence=None):\n",
    "\n",
    "    if starting_sequence is None:\n",
    "        current_sequence_input = torch.tensor([vocab.word_to_int[\"n72\"]], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    final_output_sequence = [current_sequence_input.item()]\n",
    "    \n",
    "    hidden = None\n",
    "    with torch.no_grad():\n",
    "        for i in range(sample_length):\n",
    "            #print(current_sequence_input.shape)\n",
    "            logits ,hidden = rnn(current_sequence_input.to(DEVICE), hidden)\n",
    "            logits = logits.squeeze(0)\n",
    "            probabilities = torch.softmax(logits.div(temperature), dim=1) # The less the temperature the bigger probabilities of 1 will be\n",
    "            #print(probabilities.shape)\n",
    "            #from multinomial we have [num_of_notes, 1]. But eventually we want to have [1,1,num_of_notes]\n",
    "            selected = torch.multinomial(probabilities, 1)\n",
    "            current_sequence_input = selected\n",
    "        \n",
    "            final_output_sequence.append(selected.item())\n",
    "\n",
    "    return final_output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_from_piano_rnn(rnn,vocab,sample_length=201, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = vocab.numberlized_to_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = converter2.BetterMidiToTxtConverter()\n",
    "converter.set_biggest_roll((200,128))\n",
    "converter.str_to_midi(song, \"sample3.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a2214deb2e00a4588fb64d6e2ad9e78ab07788ce628f39696990503e7a4b014"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
