{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is attempt to fix lstm.ipynb notebook with multilabel classification + Poisson regression at each timestep. I was hoping that repetitive character of the text is making problems. But it turned out that it was somethign different. The problem was again the same. Very fast convergence to returning the same and the same values again. I had also some problem with exploding values for the Poisson Loss, which was probably also due to processing sequences one by 1.\n",
    "Warning, this notebook is quite messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:\\\\GhostPresentation\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from midiToTxt import converter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import pypianoroll\n",
    "import random\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<START>\" \n",
    "END_TOKEN = \"<END>\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TEACHER_FORCING_RATIO = 0.5 #Whether or not to use teaacher forcing method \n",
    "POSITIVE_WEIGHT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "class midiTextGeneratorDataset(Dataset):\n",
    "    def __init__(self, path:str):\n",
    "        with open(path, 'r') as f:\n",
    "            corpus = f.read().strip()\n",
    "        \n",
    "        #In that case we must filter everything at first and divide tokens and counts\n",
    "        self.data = corpus.split(\"\\n\")\n",
    "        self.tokens = []\n",
    "        self.counts = []\n",
    "        \n",
    "        for i, song in enumerate(self.data):\n",
    "            self.tokens.append([])\n",
    "            self.counts.append([])\n",
    "            \n",
    "            tokenized = song.split(\" \")\n",
    "            \n",
    "            for j in range(0,len(tokenized)):\n",
    "                if j % 2 == 0:\n",
    "                    self.tokens[i].append(tokenized[j])\n",
    "                else:\n",
    "                    self.counts[i].append(int(tokenized[j]))\n",
    "        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.unique_letters = set(\"\".join([\"\".join(song) for song in self.tokens]))\n",
    "       \n",
    "        #self.unique_letters.add(END_TOKEN)\n",
    "        #self.unique_letters.add(START_TOKEN)\n",
    "        \n",
    "        self.num2word = self.get_num2word_mapping(self.unique_letters)\n",
    "        self.word2num = {v:k for k,v in self.num2word.items()}\n",
    "        \n",
    "        self.tensor_size = len(self.word2num)\n",
    "        \n",
    "    \n",
    "    def get_num2word_mapping(self, letters: set) -> dict:\n",
    "        mapping = {}\n",
    "        for index, l in enumerate(letters):\n",
    "            mapping[index] = l\n",
    "            \n",
    "        return mapping\n",
    "    \n",
    "    def token_to_tensor(self, token: str) -> torch.tensor:\n",
    "        tensor = torch.zeros((self.tensor_size, 1))\n",
    "        # if token == \"\":\n",
    "        #     return tensor\n",
    "        \n",
    "        if token == START_TOKEN:\n",
    "            tensor[self.word2num[START_TOKEN]] = 1\n",
    "        \n",
    "        elif token == END_TOKEN:\n",
    "            tensor[self.word2num[END_TOKEN]] = 1\n",
    "        \n",
    "        elif token != \"\":  \n",
    "            for letter in token:\n",
    "                tensor[self.word2num[letter]] = 1\n",
    "        \n",
    "        return torch.unsqueeze(tensor.view(1, -1),0) # we want [1, 1, num_of_characters]\n",
    "    \n",
    "    def tensor_to_token(self, tensor: torch.tensor) -> str:\n",
    "        notes = tensor.nonzero()\n",
    "        token = \"\"\n",
    "        for _, note in notes:\n",
    "            token += self.num2word[note.item()]\n",
    "            if len(token) > 10:\n",
    "                break\n",
    "            \n",
    "        return token\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        song = list(self.tokens[idx]) #MUST DO A COPY HERE SINCE WE THEN MODIFY THAT ARRAY!!!\n",
    "        \n",
    "        #song.insert(0, START_TOKEN)\n",
    "        #song.append(END_TOKEN)\n",
    "        \n",
    "        inp = self.token_to_tensor(song[0])\n",
    "        #print(inp.shape)\n",
    "        for token in song[1:]:\n",
    "            t = self.token_to_tensor(token)\n",
    "            #print(t.shape)\n",
    "            inp = torch.cat((inp, t), dim=0)\n",
    "        \n",
    "        counts = list(self.counts[idx])\n",
    "        #counts.insert(0, 0)\n",
    "        #counts.append(0)\n",
    "        \n",
    "        #print(song)\n",
    "        #print(counts)\n",
    "        #self.print_given_song(idx)\n",
    "        \n",
    "        return inp, torch.tensor(counts)\n",
    "    \n",
    "    def print_given_song(self, idx):\n",
    "        print(len(self.data[idx].split(\" \")))\n",
    "        print(self.data[idx])\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(output_size + 1, hidden_size) # since we have notes played + number of times it happened\n",
    "        self.out_classification = nn.Linear(hidden_size, output_size)\n",
    "        self.out_counts = nn.Linear(hidden_size, 1) # What this would yield is logarithm of expected value of poisson distribution\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.view(1,1,-1)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        #print(output.shape)\n",
    "        output_classification = self.out_classification(output[0])\n",
    "        output_counts = self.out_counts(output[0])\n",
    "        \n",
    "        return output_classification, output_counts, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path: str):\n",
    "    model = DecoderRNN(256, dataset.tensor_size)\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input, counts, decoder, decoder_optimizer, criterion_class, criterion_count, dataset, acceptance_level = 0.5):\n",
    "    #Input already contains start and end tokens!\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    target_length = input.shape[0]\n",
    "    if target_length == 0:\n",
    "        print(\"Problem!\")\n",
    "    #print(target_length)\n",
    "    loss_class = 0\n",
    "    loss_count = 0\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "    \n",
    "    decoder_input = torch.cat((input[0], counts[0].view(1,-1)), dim=1) #merging first chord and number of it's ocurences\n",
    "    #print(decoder_input.shape)\n",
    "    decoder_hidden = decoder.initHidden()\n",
    "    if use_teacher_forcing:\n",
    "        #print(\"TEACHER FORCING\")\n",
    "        for i in range(1, target_length):\n",
    "            decoder_output_class, decoder_output_count, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            # print(f\"GIVEN INPUT: {dataset.tensor_to_token(decoder_input.detach())}\")\n",
    "            # print(f\"DECODER OUTPUT: {dataset.tensor_to_token((decoder_output_class > acceptance_level).type(dtype=torch.float32).detach())}\")\n",
    "            \n",
    "            loss_class += criterion_class(decoder_output_class, input[i]) # Based on i-1 chord does model predicted ith chord?\n",
    "            loss_count += criterion_count(decoder_output_count, counts[i])\n",
    "            \n",
    "            decoder_input = torch.cat((input[i], counts[i].view(1,-1)), dim=1) # i ths chord is input to the next iteration\n",
    "        loss_class.backward(retain_graph=True) #During first iteration I retain computational graph to be able to do backward once more\n",
    "        loss_count.backward() #Now I cen delete the graph \n",
    "    else:\n",
    "        #print(\"NOT TEACHER FORCING\")\n",
    "        for i in range(1, target_length):\n",
    "            decoder_output_class, decoder_output_count, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            \n",
    "            # print(f\"GIVEN INPUT: {dataset.tensor_to_token(decoder_input.detach())}\")\n",
    "            # print(f\"DECODER OUTPUT: {dataset.tensor_to_token((decoder_output_class > acceptance_level).type(dtype=torch.float32).detach())}\")\n",
    "            \n",
    "            loss_class += criterion_class(decoder_output_class, input[i]) # Based on i-1 chord does model predicted ith chord?\n",
    "            \n",
    "            loss_count += criterion_count(decoder_output_count, counts[i])\n",
    "            \n",
    "            \n",
    "            chords_predicted = (decoder_output_class > acceptance_level).type(dtype=torch.float32).detach()\n",
    "            decoder_input = torch.cat((chords_predicted, counts[i]), dim=1) \n",
    "            \n",
    "        loss_class.backward()\n",
    "        loss_count.backward() #Now I cen delete the graph   \n",
    "            #THERE also is a problem with this loss_count, since it sometimes may be very big. And network became unstable in a moment!\n",
    "            #Since this is propated through entire network entire network became a trash in a moment\n",
    "            \n",
    "            \n",
    "            #This is problably not the best idea, let it go through whole target. Not to stop too early\n",
    "            #if decoder_input[0, dataset.word2num[END_TOKEN]] == 1:\n",
    "            #    break\n",
    "            \n",
    "    #print(loss_class, loss_count)\n",
    "    #loss = loss_class + loss_count #! WE CAN'T SUM UM THESE LOSSES Loss_count basically can take as big negative numbers as we wish!!!!!\n",
    "    #loss_class.backward(retain_graph=True) #During first iteration I retain computational graph to be able to do backward once more\n",
    "    #loss_count.backward() #Now I cen delete the graph\n",
    "    if torch.isnan(loss_class):\n",
    "        print(\"NAN!! CLASS\")\n",
    "    #if torch.isnan(loss_count):\n",
    "    #    print(\"NAN!! COUNT\")\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss_class.item() / target_length,0#, loss_count.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, decoder, decoder_optimizer, criterion_class, criterion_count,dataset):\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    losses_class = []\n",
    "    losses_count = []\n",
    "    for batch_idx, (input, count) in enumerate(loop):\n",
    "        input, count = input.to(DEVICE), count.to(DEVICE)\n",
    "        #print(input.shape)\n",
    "        loss_class, loss_count = train(input[0], count[0], decoder, decoder_optimizer, criterion_class, criterion_count,dataset)\n",
    "        losses_class.append(loss_class)\n",
    "        losses_count.append(loss_count)\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss_class=loss_class, loss_count=loss_count)\n",
    "        #break\n",
    "        \n",
    "    #print(losses_class)\n",
    "    #print(losses_count)\n",
    "    return sum(losses_class) / len(losses_class), sum(losses_count)/ len(losses_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../lstm_dataset_compressed_very_small.txt\"\n",
    "\n",
    "dataset = midiTextGeneratorDataset(path)\n",
    "dataloader = DataLoader(dataset, batch_size=1,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderRNN(256, dataset.tensor_size)\n",
    "decoder.to(DEVICE)\n",
    "\n",
    "criterion_class = torch.nn.BCEWithLogitsLoss(pos_weight=torch.full((dataset.tensor_size,), POSITIVE_WEIGHT, device=DEVICE)) # This loss combines a Sigmoid layer and the BCELoss in one single class\n",
    "#Which is better for numerical stability.\n",
    "criterion_count = torch.nn.PoissonNLLLoss(log_input=True) # So this loss expect log(lambda) = x * b. Then it transforms it using exp\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 15.10it/s, loss_class=1.77, loss_count=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss_class: 2.3976305463973953, loss_count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 15.29it/s, loss_class=1.91, loss_count=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss_class: 2.2592174983187103, loss_count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 15.46it/s, loss_class=1.67, loss_count=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss_class: 1.936190850302386, loss_count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 15.19it/s, loss_class=1.41, loss_count=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss_class: 1.7643650039720982, loss_count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 15.42it/s, loss_class=2.31, loss_count=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss_class: 1.7220677185617645, loss_count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 15.43it/s, loss_class=2.16, loss_count=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss_class: 1.7279601062947803, loss_count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 14.97it/s, loss_class=2.13, loss_count=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss_class: 1.718886055677033, loss_count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 14.51it/s, loss_class=2.06, loss_count=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss_class: 1.715828314823648, loss_count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 14.63it/s, loss_class=1.63, loss_count=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss_class: 1.715799369111969, loss_count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 14.49it/s, loss_class=1.51, loss_count=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss_class: 1.7119448720091064, loss_count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 6/23 [00:00<00:01, 11.69it/s, loss_class=2.29, loss_count=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18412/755395439.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mloss_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_count\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {i}, loss_class: {loss_class}, loss_count: {loss_count}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# torch.save({\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#         'epoch': i,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18412/1427759311.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(dataloader, decoder, decoder_optimizer, criterion_class, criterion_count, dataset)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m#print(input.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mloss_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mlosses_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mlosses_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18412/1718420931.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(input, counts, decoder, decoder_optimizer, criterion_class, criterion_count, dataset, acceptance_level)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchords_predicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts_predicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mloss_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m#THERE also is a problem with this loss_count, since it sometimes may be very big. And network became unstable in a moment!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    loss_class, loss_count =train_epoch(dataloader, decoder, optimizer, criterion_class, criterion_count,dataset)\n",
    "    print(f\"Epoch {i}, loss_class: {loss_class}, loss_count: {loss_count}\")\n",
    "    # torch.save({\n",
    "    #         'epoch': i,\n",
    "    #         'model_state_dict': decoder.state_dict(),\n",
    "    #         'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #         'loss': 0,\n",
    "    #         }, f\"model_{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-200*math.log(200)+200 # This loss behaves weirdly a little bit. Since it doesn't get to 0 when prediction is correct but rather takes negative value! However it takes minimum value for the correct solution\n",
    "#Basically we are doing good. So we are minimizing this. However this doesn't have minimum equal to 0! So we can't sum up loss from classification and Poisson regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = midiTextGeneratorDataset(path)\n",
    "\n",
    "#decoder = load_model(\"model_8.pt\")\n",
    "song = \"7LOSUX\"\n",
    "decoder_input = dataset.token_to_tensor(song).to(DEVICE)\n",
    "decoder_input = torch.cat((decoder_input[0], torch.tensor([1], device = DEVICE, dtype=torch.float32).view(1,-1)), dim=1)\n",
    "song += \" 1\"\n",
    "finished = False\n",
    "decoder_hidden = decoder.initHidden().to(DEVICE)\n",
    "i = 0\n",
    "prob = 0.5\n",
    "decoder.eval()\n",
    "while not finished:\n",
    "    decoder_token, decoder_count, decoder_hidden = decoder(\n",
    "        decoder_input, decoder_hidden)\n",
    "    \n",
    "    decoder_token = torch.sigmoid(decoder_token)\n",
    "    #print(decoder_count)\n",
    "    #print(decoder_token)\n",
    "    #print(decoder_output)\n",
    "    decoder_token = (decoder_token > prob).type(dtype=torch.float32)\n",
    "    #print(decoder_token)\n",
    "    i+=1\n",
    "    #break\n",
    "    token = dataset.tensor_to_token(decoder_token)\n",
    "\n",
    "    decoder_input = decoder_token # i ths chord is input to the next iteration\n",
    "        \n",
    "    # if i == 1:\n",
    "    #     song = token\n",
    "    # else:\n",
    "    song += \" \" + token\n",
    "    \n",
    "    #print(decoder_count[0])\n",
    "    lambda_ = np.exp(decoder_count[0].item())\n",
    "    repetitions = max([1, np.random.poisson(lambda_,1)])\n",
    "    \n",
    "    song += \" \" + str(int(repetitions))\n",
    "    # if i % 10 == 0:\n",
    "    #     decoder_input = dataset.token_to_tensor(\"\").to(DEVICE)\n",
    "    if i % 30 == 0:\n",
    "         decoder_hidden = decoder.initHidden().to(DEVICE)\n",
    "    if i == 200:\n",
    "        break\n",
    "    \n",
    "    decoder_input = torch.cat((decoder_input, torch.tensor([repetitions], device = DEVICE, dtype=torch.float32).view(1,-1)), dim=1)\n",
    "    #decoder_input = decoder_output # i ths chord is input to the next iteration\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7LOSUX 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 2 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 4 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 4 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 4 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 2 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 2 8ICO=:TU0EG 2 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 2 8ICO=:TU0EG 2 8ICO=:TU0EG 3 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1 8ICO=:TU0EG 1'"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midiToTxt import converter, compressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = compressor.decompress(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "song\n",
    "\n",
    "c = converter.MidiTxtConverter()\n",
    "\n",
    "track =c.string_to_multitrack(song.strip())\n",
    "\n",
    "pypianoroll.write(\"lstm3.mid\", track)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a2214deb2e00a4588fb64d6e2ad9e78ab07788ce628f39696990503e7a4b014"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
