{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First baseline approach in which we perform multilabel classification at each timestep. It differs from lstm.ipynb and lstm2.ipynb with different approach to training. Here I create big batches and do not backpropagate after processing every sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_BATCH_SIZE = 60\n",
    "VAL_BATCH_SIZE = 30\n",
    "DATA_PATH = '../data/Nottingham/' # I was using very small dataset here. Just to check if this approach work and don't spend \n",
    "#Hours on training just to understand the approach is wrong\n",
    "NUM_EPOCHS = 5\n",
    "POSITIVE_WEIGHT = 2 # Since it is more likely not to play a note. I've introduced some small positive weight, so that model\n",
    "#doesn't converge to predicting all 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_pianoroll(path, resolution = 8):\n",
    "    #The bigger resolution will be the more detailed the representation, but also the longer sequences will become.\n",
    "    #So it may be hard for the LSTM to handle them. \n",
    "    midi_data = pypianoroll.read(path, resolution=resolution)\n",
    "    \n",
    "    piano_roll = midi_data.blend()[:, 21:109] #Taking just 88 useful notes. This will have shape\n",
    "    #[length_of_sequence, number_of_notes]\n",
    "    \n",
    "    #we want to perform multilabel classification at each step so we need to binaryze the roll\n",
    "    piano_roll[piano_roll > 0] = 1\n",
    "    \n",
    "    return piano_roll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    #Helper function for DataLoader\n",
    "    #Batch is a list of tuple in the form (input, target)\n",
    "    #We do not have to padd everything thanks to pack_sequence\n",
    "    #!Using this function we decide how batches are prepared\n",
    "    data = [item[0] for item in batch] #\n",
    "    data = nn.utils.rnn.pack_sequence(data, enforce_sorted=False) # we prepare batch as a packed_sequence.\n",
    "    #This function is very cool as we do not need to pad these sequences\n",
    "    targets = [item[1] for item in batch]\n",
    "    targets = nn.utils.rnn.pack_sequence(targets, enforce_sorted=False)\n",
    "    return [data, targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesGenerationDataset(data.Dataset):\n",
    "    \"\"\"I've decided not to work on text and convert it to the piano roll since this only makes more work. We can\n",
    "       work directly on the pianoroll, and if needed convert it to text representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.full_filenames = []\n",
    "        \n",
    "        #Here we assume that all midi files are valid, we do not check anything here.\n",
    "        for root, subdirs, files in os.walk(path):\n",
    "            for f in files:\n",
    "                self.full_filenames.append(os.path.join(root, f))\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.full_filenames)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        full_filename = self.full_filenames[index]\n",
    "        \n",
    "        piano_roll = path_to_pianoroll(full_filename)\n",
    "        \n",
    "        #input and gt are shifted by one step w.r.t one another.\n",
    "        input_sequence = piano_roll[:-1, :]\n",
    "        ground_truth_sequence = piano_roll[1:, :]\n",
    "        \n",
    "        return torch.tensor(input_sequence, dtype=torch.float32), torch.tensor(ground_truth_sequence, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NotesGenerationDataset(os.path.join(DATA_PATH, \"train\"))\n",
    "\n",
    "#ofc we want big batch_size. However, one training sample takes quite a lot of memory.\n",
    "#We will use torch.cuda.amp.autocast() so that we can make bigger batches\n",
    "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE,\n",
    "                                              shuffle=True, drop_last=True, collate_fn=collate)\n",
    "\n",
    "valset = NotesGenerationDataset(os.path.join(DATA_PATH, \"valid\"))\n",
    "\n",
    "valset_loader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small sanity check that our sets do not intersect at any moment\n",
    "train_songs = set(trainset.full_filenames)\n",
    "for song in valset.full_filenames:\n",
    "    assert not song in train_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size # amount of different notes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #At first we need layer that will encode our vector with only once to better representation\n",
    "        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        #At the end we want to get vector with logits of all notes\n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        #During training the input is packedSequence, but during inference this will be just a tensor\n",
    "        if isinstance(inp, nn.utils.rnn.PackedSequence):\n",
    "            #If we have Packed sequence we proceed a little bit differently\n",
    "            batch_sizes = inp.batch_sizes\n",
    "            notes_encoded = self.notes_encoder(inp.data) #PackedSequence.data is a tensor representation of shape [samples, num_of_notes]\n",
    "            rnn_in = nn.utils.rnn.PackedSequence(notes_encoded,batch_sizes) #This is not recommended in PyTorch documentation.\n",
    "            #However this saves a day here. Since otherwise we would have to create padded sequences \n",
    "            outputs, hidden = self.lstm(rnn_in, hidden)\n",
    "            \n",
    "            logits = self.logits_fc(outputs.data) #Again we go from packedSequence to tensor.\n",
    "        else:\n",
    "            #If we have tensor at the input this is pretty straightforward\n",
    "            notes_encoded = self.notes_encoder(inp)\n",
    "            outputs, hidden = self.lstm(notes_encoded, hidden)\n",
    "            logits = self.logits_fc(outputs)\n",
    "        \n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:97: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Now sanity check about Packed Sequences. So I check if Unpacking -> packing the packed Sequence will lead to exactly the same Object.\n",
    "inp, targets = next(iter(trainset_loader))\n",
    "\n",
    "batch_sizes = inp.batch_sizes\n",
    "inp2 = nn.utils.rnn.PackedSequence(inp.data, batch_sizes)\n",
    "assert torch.all(torch.eq(inp.data, inp2.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=88, hidden_size=256, num_classes=88)\n",
    "rnn = rnn.to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.full((88,), POSITIVE_WEIGHT, device=DEVICE))\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    rnn.eval()\n",
    "    loop = tqdm(valset_loader, leave=True)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inp, target) in enumerate(loop):\n",
    "            inp, target = inp.to(DEVICE), target.to(DEVICE)\n",
    "            logits, _ = rnn(inp)\n",
    "\n",
    "            loss = criterion(logits, target.data)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    rnn.train()\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:55<00:00,  5.05s/it, loss=0.238]\n",
      "100%|██████████| 6/6 [00:29<00:00,  4.87s/it, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 0.5099352300167084, val_loss: 0.2149465282758077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:39<00:00,  3.55s/it, loss=0.161]\n",
      "100%|██████████| 6/6 [00:07<00:00,  1.32s/it, loss=0.166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 0.17619910023429178, val_loss: 0.15660164753595987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:30<00:00,  2.74s/it, loss=0.149]\n",
      "100%|██████████| 6/6 [00:07<00:00,  1.21s/it, loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_loss: 0.1536434834653681, val_loss: 0.15122093260288239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:30<00:00,  2.79s/it, loss=0.151]\n",
      "100%|██████████| 6/6 [00:08<00:00,  1.37s/it, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train_loss: 0.15057085183533755, val_loss: 0.14899204423030218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:31<00:00,  2.86s/it, loss=0.149]\n",
      "100%|██████████| 6/6 [00:07<00:00,  1.29s/it, loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train_loss: 0.1496228358962319, val_loss: 0.14815168579419455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip = 1.0 #with Rnn's batch normalization is tricky to implement so instead we can use gradient clipping\n",
    "#but just the clipping may be not enough, so we perform kind of normalization too\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "loss_list = []\n",
    "val_list = []\n",
    "\n",
    "for epoch_number in range(NUM_EPOCHS):\n",
    "    loop = tqdm(trainset_loader, leave=True)\n",
    "    losses = []\n",
    "    for idx, (inp, target) in enumerate(loop):\n",
    "        \n",
    "        inp, target = inp.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad() # remember to do this every time not to accumulate gradient\n",
    "\n",
    "        with torch.cuda.amp.autocast(): \n",
    "            logits, _ = rnn(inp)\n",
    "            loss = criterion(logits, target.data)\n",
    "             \n",
    "        scaler.scale(loss).backward()\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "        torch.nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    train_loss = sum(losses)/len(losses)\n",
    "    loss_list.append(train_loss)\n",
    "    current_val_loss = validate()\n",
    "    val_list.append(current_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch_number}, train_loss: {train_loss}, val_loss: {current_val_loss}\")\n",
    "    if current_val_loss < best_val_loss:\n",
    "        \n",
    "        torch.save(rnn.state_dict(), 'music_rnn.pth')\n",
    "        best_val_loss = current_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_piano_rnn(sample_length=4, temperature=1, starting_sequence=None):\n",
    "\n",
    "    #Sem some default starting sequence if noone was given\n",
    "    if starting_sequence is None:   \n",
    "        current_sequence_input = torch.zeros(1,1, 88, dtype=torch.float32, device=DEVICE)\n",
    "        current_sequence_input[0, 0, 40] = 1\n",
    "        current_sequence_input[0, 0, 50] = 1\n",
    "        current_sequence_input[0, 0, 56] = 1\n",
    "\n",
    "    final_output_sequence = [current_sequence_input.squeeze(1)]\n",
    "    \n",
    "    hidden = None\n",
    "    with torch.no_grad():\n",
    "        for i in range(sample_length):\n",
    "\n",
    "            output, hidden = rnn(current_sequence_input, hidden)\n",
    "            \n",
    "            #By dividing by temperature before passing it to the sigmoid we can either make it more peaked\n",
    "            #or more uniform. It works because rate of change of sigmoid is not linear w.r.t input.\n",
    "            #So changing from 0.01 to 0.1 won't make that big difference But change from 0.1 to 1 make a difference of about 25%\n",
    "            probabilities = torch.sigmoid(output.div(temperature))\n",
    "           \n",
    "            prob_of_0 = 1 - probabilities\n",
    "            dist = torch.stack((prob_of_0, probabilities), dim=3).squeeze() #Here we will get tensor [num_of_notes, 2]\n",
    "            \n",
    "            #from multinomial we have [num_of_notes, 1]. But eventually we want to have [1,1,num_of_notes]\n",
    "            current_sequence_input = torch.multinomial(dist, 1).squeeze().unsqueeze(0).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "            final_output_sequence.append(current_sequence_input.squeeze(1))\n",
    "\n",
    "    sampled_sequence = torch.cat(final_output_sequence, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sampled_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_from_piano_rnn(sample_length=200, temperature=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(sample) # Just to check how many notes are played withing these 200 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll = np.zeros((201,128))\n",
    "roll[:, 21:109] = sample\n",
    "roll[roll == 1] = 100\n",
    "track = pypianoroll.Multitrack(resolution=3)\n",
    "track.append(pypianoroll.StandardTrack(pianoroll=roll))\n",
    "pypianoroll.write(\"baseline1_song2.mid\", track)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a2214deb2e00a4588fb64d6e2ad9e78ab07788ce628f39696990503e7a4b014"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
