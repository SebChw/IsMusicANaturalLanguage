{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_BATCH_SIZE = 60\n",
    "VAL_BATCH_SIZE = 30\n",
    "DATA_PATH = '../data/lmd_aligned/'\n",
    "NUM_EPOCHS = 5\n",
    "POSITIVE_WEIGHT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_pianoroll(path):\n",
    "    #Resolution is set to 3 so that the sequences are not that long\n",
    "    midi_data = pypianoroll.read(path, resolution=3)\n",
    "    \n",
    "    piano_roll = midi_data.blend()[:, 21:109].transpose() #Taking just 81 usefull notes\n",
    "    \n",
    "    #we want to perform multilabel classification at each step so we need to binaryze the roll\n",
    "    piano_roll[piano_roll > 0] = 1\n",
    "    \n",
    "    return piano_roll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    #Helper function for DataLoader\n",
    "    #Batch is a list of tuple in the form (input, target)\n",
    "    #We do not have to padd everything thanks to pack_sequence\n",
    "    data = [item[0] for item in batch] #\n",
    "    data = nn.utils.rnn.pack_sequence(data, enforce_sorted=False)\n",
    "    targets = [item[1] for item in batch]\n",
    "    targets = nn.utils.rnn.pack_sequence(targets, enforce_sorted=False)\n",
    "    return [data, targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_of_samples(path: str) -> int:\n",
    "    num_of_samples = 0\n",
    "    for root, subdirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            num_of_samples += 1\n",
    "            \n",
    "    return num_of_samples\n",
    "\n",
    "NUM_OF_SAMPLES = calculate_num_of_samples(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesGenerationDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, train=True, num_of_samples=NUM_OF_SAMPLES, train_percentage = 0.8):\n",
    "        \n",
    "        self.path = path\n",
    "        self.num_of_samples = num_of_samples\n",
    "        self.full_filenames = []\n",
    "        self.train_percentage = train_percentage\n",
    "        \n",
    "        #Here we assume that all midi files are valid, we do not check anything here.\n",
    "        train_dataset_volume = num_of_samples*train_percentage\n",
    "        if train:\n",
    "            for root, subdirs, files in os.walk(path):\n",
    "                for f in files:\n",
    "                    self.full_filenames.append(os.path.join(root, f))\n",
    "                    if len(self.full_filenames) >= train_dataset_volume:\n",
    "                        return\n",
    "        else:\n",
    "            skipped = 0 # we want to skip bigger part of the dataset\n",
    "            for root, subdirs, files in os.walk(path):\n",
    "                for f in files:\n",
    "                    if skipped < train_dataset_volume:\n",
    "                        skipped +=1\n",
    "                    else:\n",
    "                        self.full_filenames.append(os.path.join(root, f))\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.full_filenames)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        full_filename = self.full_filenames[index]\n",
    "        \n",
    "        piano_roll = path_to_pianoroll(full_filename)\n",
    "        \n",
    "        #input and gt are shifted by one step w.r.t one another.\n",
    "        #we transpose it since piano_roll has shape [num_of_notes, number_of_event] we want to have format [number of events, num_of_notes]\n",
    "        input_sequence = piano_roll[:, :-1].transpose()\n",
    "        ground_truth_sequence = piano_roll[:, 1:].transpose()\n",
    "        \n",
    "        return torch.tensor(input_sequence, dtype=torch.float32), torch.tensor(ground_truth_sequence, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NotesGenerationDataset(DATA_PATH, train=True)\n",
    "\n",
    "#ofc we want big batch_size. However, one training sample takes quite a lot of memory.\n",
    "#We will use torch.cuda.amp.autocast() so that we can make bigger batches\n",
    "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE,\n",
    "                                              shuffle=True, drop_last=True, collate_fn=collate)\n",
    "\n",
    "valset = NotesGenerationDataset(DATA_PATH, train=False)\n",
    "\n",
    "valset_loader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small sanity check that our sets do not intersect at any moment\n",
    "train_songs = set(trainset.full_filenames)\n",
    "for song in valset.full_filenames:\n",
    "    assert not song in train_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size # amount of different notes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #At first we need layer that will encode our vector with only once to better representation\n",
    "        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        #At the end we want to get vector with logits of all notes\n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        \n",
    "        if isinstance(inp, nn.utils.rnn.PackedSequence):\n",
    "            #If we have Packed sequence we proceed a little bit differently\n",
    "            batch_sizes = inp.batch_sizes\n",
    "            notes_encoded = self.notes_encoder(inp.data) #PackedSequence.data is a tensor representation of shape [samples, num_of_notes]\n",
    "            rnn_in = nn.utils.rnn.PackedSequence(notes_encoded,batch_sizes) #This is not recommended in PyTorch documentation.\n",
    "            #However this saves a day here. Since otherwise we would have to create padded sequences \n",
    "            outputs, hidden = self.lstm(rnn_in, hidden)\n",
    "            \n",
    "            logits = self.logits_fc(outputs.data) #Again we go from packedSequence to tensor.\n",
    "        else:\n",
    "            #If we have tensor at the input this is pretty straightforward\n",
    "            notes_encoded = self.notes_encoder(inp)\n",
    "            outputs, hidden = self.lstm(notes_encoded, hidden)\n",
    "            logits = self.logits_fc(outputs)\n",
    "        \n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\miniconda3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:97: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Now sanity check about Packed Sequences. So I check if Unpacking -> packing the packed Sequence will lead to exactly the same Object.\n",
    "inp, targets = next(iter(trainset_loader))\n",
    "\n",
    "batch_sizes = inp.batch_sizes\n",
    "inp2 = nn.utils.rnn.PackedSequence(inp.data, batch_sizes)\n",
    "assert torch.all(torch.eq(inp.data, inp2.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=88, hidden_size=256, num_classes=88)\n",
    "rnn = rnn.to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.full((88,), POSITIVE_WEIGHT, device=DEVICE))\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    rnn.eval()\n",
    "    loop = tqdm(valset_loader, leave=True)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inp, target) in enumerate(loop):\n",
    "            inp, target = inp.to(DEVICE), target.to(DEVICE)\n",
    "            logits, _ = rnn(inp)\n",
    "\n",
    "            loss = criterion(logits, target.data)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    rnn.train()\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:19<00:00,  1.54s/it, loss=0.205]\n",
      "100%|██████████| 7/7 [00:04<00:00,  1.49it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 0.47092414590028614, val_loss: 0.18847319058009557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:17<00:00,  1.35s/it, loss=0.149]\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.85it/s, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 0.16357416143784156, val_loss: 0.15431699369634902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:16<00:00,  1.27s/it, loss=0.148]\n",
      "100%|██████████| 7/7 [00:04<00:00,  1.73it/s, loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_loss: 0.15196926433306474, val_loss: 0.15230111352034978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:15<00:00,  1.18s/it, loss=0.144]\n",
      "100%|██████████| 7/7 [00:04<00:00,  1.61it/s, loss=0.16] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train_loss: 0.15008225234655234, val_loss: 0.15109907516411372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:20<00:00,  1.61s/it, loss=0.151]\n",
      "100%|██████████| 7/7 [00:05<00:00,  1.27it/s, loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train_loss: 0.14965867537718552, val_loss: 0.1498615379844393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip = 1.0\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "loss_list = []\n",
    "val_list = []\n",
    "\n",
    "for epoch_number in range(NUM_EPOCHS):\n",
    "    loop = tqdm(trainset_loader, leave=True)\n",
    "    losses = []\n",
    "    for idx, (inp, target) in enumerate(loop):\n",
    "        \n",
    "        inp, target = inp.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(): \n",
    "            logits, _ = rnn(inp)\n",
    "            loss = criterion(logits, target.data)\n",
    "             \n",
    "        scaler.scale(loss).backward()\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "        torch.nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    train_loss = sum(losses)/len(losses)\n",
    "    loss_list.append(train_loss)\n",
    "    current_val_loss = validate()\n",
    "    val_list.append(current_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch_number}, train_loss: {train_loss}, val_loss: {current_val_loss}\")\n",
    "    if current_val_loss < best_val_loss:\n",
    "        \n",
    "        torch.save(rnn.state_dict(), 'music_rnn.pth')\n",
    "        best_val_loss = current_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_piano_rnn(sample_length=4, temperature=1, starting_sequence=None, deterministic = False, threshold=0.5):\n",
    "\n",
    "    if starting_sequence is None:\n",
    "                \n",
    "        current_sequence_input = torch.zeros(1,1, 88, dtype=torch.float32, device=DEVICE)\n",
    "        current_sequence_input[0, 0, 40] = 1\n",
    "        current_sequence_input[0, 0, 50] = 1\n",
    "        current_sequence_input[0, 0, 56] = 1\n",
    "\n",
    "    final_output_sequence = [current_sequence_input.squeeze(1)]\n",
    "    \n",
    "    hidden = None\n",
    "    with torch.no_grad():\n",
    "        for i in range(sample_length):\n",
    "\n",
    "            output, hidden = rnn(current_sequence_input, hidden)\n",
    "            \n",
    "            probabilities = torch.sigmoid(output.div(temperature)) # The less the temperature the bigger probabilities of 1 will be\n",
    "            if deterministic and len(final_output_sequence) > 5:\n",
    "                #print(probabilities)\n",
    "                current_sequence_input = (probabilities > threshold).to(torch.float32)                \n",
    "            else:\n",
    "                #print(probabilities)\n",
    "                prob_of_0 = 1 - probabilities\n",
    "                #print(prob_of_0)\n",
    "                dist = torch.stack((prob_of_0, probabilities), dim=3).squeeze() #Here we will get tensor [num_of_notes, 2]\n",
    "                #print(dist)\n",
    "                \n",
    "                #from multinomial we have [num_of_notes, 1]. But eventually we want to have [1,1,num_of_notes]\n",
    "                current_sequence_input = torch.multinomial(dist, 1).squeeze().unsqueeze(0).unsqueeze(1).to(torch.float32)\n",
    "                #print(current_sequence_input)\n",
    "                #break\n",
    "\n",
    "            final_output_sequence.append(current_sequence_input.squeeze(1))\n",
    "\n",
    "    sampled_sequence = torch.cat(final_output_sequence, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sampled_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_from_piano_rnn(sample_length=200, temperature=0.25, deterministic=False, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541.0"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll = np.zeros((201,128))\n",
    "roll[:, 21:109] = sample\n",
    "roll[roll == 1] = 100\n",
    "track = pypianoroll.Multitrack(resolution=3)\n",
    "track.append(pypianoroll.StandardTrack(pianoroll=roll))\n",
    "pypianoroll.write(\"sample2.mid\", track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a2214deb2e00a4588fb64d6e2ad9e78ab07788ce628f39696990503e7a4b014"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
