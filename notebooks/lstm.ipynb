{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This notebook has an approach that failed to give any satisfying results. It was highly inspired from PyTorch's tutorial https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html. Basically network converged very fast to just rewriting the last input. I was not sure why it was the case. I was using batches of size 1 here, and kind of very sequential learning as they did in the tutorial. Probably it lead to failure. Also very big resolution which I've used there didn't help<br>\n",
    " Warning, this notebook is quite messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from midiToTxt import converter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import pypianoroll\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class midiTextGeneratorDataset(Dataset):\n",
    "    def __init__(self, corpus:str):\n",
    "        self.unique_letters = set(corpus)\n",
    "        self.unique_letters.remove(\"\\n\")\n",
    "        \n",
    "        self.unique_letters.add(START_TOKEN)\n",
    "        self.unique_letters.add(END_TOKEN)\n",
    "        \n",
    "        self.num2word = self.get_num2word_mapping(self.unique_letters)\n",
    "        self.word2num = {v:k for k,v in self.num2word.items()}\n",
    "        \n",
    "        self.tensor_size = len(self.word2num)\n",
    "        \n",
    "        self.data = corpus.split(\"\\n\")\n",
    "    \n",
    "    def get_num2word_mapping(self, letters: set) -> dict:\n",
    "        mapping = {}\n",
    "        for index, l in enumerate(letters):\n",
    "            mapping[index] = l\n",
    "            \n",
    "        return mapping\n",
    "    \n",
    "    def token_to_tensor(self, token: str) -> torch.tensor:\n",
    "        tensor = torch.zeros((self.tensor_size, 1))\n",
    "        if token == START_TOKEN:\n",
    "            tensor[self.word2num[START_TOKEN]] = 1\n",
    "        \n",
    "        elif token == END_TOKEN:\n",
    "            tensor[self.word2num[END_TOKEN]] = 1\n",
    "        \n",
    "        else:  \n",
    "            for letter in token:\n",
    "                tensor[self.word2num[letter]] = 1\n",
    "        \n",
    "        return torch.unsqueeze(tensor.view(1, -1),0) # we want [1, 1, num_of_characters]\n",
    "    \n",
    "    def tensor_to_token(self, tensor: torch.tensor) -> str:\n",
    "        notes = tensor.nonzero()\n",
    "        token = \"\"\n",
    "        for _, note in notes:\n",
    "            token += self.num2word[note.item()]\n",
    "            if len(token) > 10:\n",
    "                break\n",
    "            \n",
    "        return token\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        song = self.data[idx]\n",
    "        song = song.split(\" \")\n",
    "        song.insert(0, START_TOKEN)\n",
    "        song.append(END_TOKEN)\n",
    "        \n",
    "        inp = self.token_to_tensor(song[0])\n",
    "        #print(inp.shape)\n",
    "        for token in song[1:]:\n",
    "            t = self.token_to_tensor(token)\n",
    "            #print(t.shape)\n",
    "            inp = torch.cat((inp, t), dim=0)\n",
    "        \n",
    "        return inp\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(output_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.view(1,1,-1)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        #print(output.shape)\n",
    "        output = self.out(output[0])\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input, decoder, decoder_optimizer, criterion):\n",
    "    #Input already contains start and end tokens!\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    target_length = input.shape[0]\n",
    "    #print(target_length)\n",
    "    loss = 0\n",
    "    \n",
    "    teacher_forcing = True\n",
    "    decoder_input = input[0] #Taking START token out\n",
    "    decoder_hidden = decoder.initHidden()\n",
    "    for i in range(1, target_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        \n",
    "        loss += criterion(decoder_output, input[i]) # Based on i-1 chord does model predicted ith chord?\n",
    "        #print(loss)\n",
    "        decoder_input = input[i] # i ths chord is input to the next iteration\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, decoder, decoder_optimizer, criterion):\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "\n",
    "    for batch_idx, (input) in enumerate(loop):\n",
    "        input = input.to(DEVICE)\n",
    "        #print(input.shape)\n",
    "        loss = train(input[0], decoder, decoder_optimizer, criterion)\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"lstm_dataset.txt\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = midiTextGeneratorDataset(corpus)\n",
    "dataloader = DataLoader(dataset, batch_size=1,\n",
    "                shuffle=True)\n",
    "    \n",
    "decoder = DecoderRNN(256, dataset.tensor_size)\n",
    "decoder.to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.full((dataset.tensor_size,),20, device=DEVICE)) # This loss combines a Sigmoid layer and the BCELoss in one single class\n",
    "#Which is better for numerical stability.\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2443/2443 [08:31<00:00,  4.78it/s, loss=0.162] \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_epoch(dataloader, decoder, optimizer, criterion)\n",
    "    torch.save({\n",
    "            'epoch': i,\n",
    "            'model_state_dict': decoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': 0,\n",
    "            }, \"model.pt\")\n",
    "    break\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to generate something\n",
    "dataset = midiTextGeneratorDataset(corpus)\n",
    "\n",
    "decoder.eval()\n",
    "song = \"4CL\"\n",
    "decoder_input = dataset.token_to_tensor(song).to(DEVICE)\n",
    "finished = False\n",
    "decoder_hidden = decoder.initHidden().to(DEVICE)\n",
    "i = 0\n",
    "prob = 0.7\n",
    "previous = \"\"\n",
    "repeated = 0\n",
    "while not finished:\n",
    "    decoder_output, decoder_hidden = decoder(\n",
    "        decoder_input, decoder_hidden)\n",
    "    \n",
    "    decoder_output = torch.sigmoid(decoder_output)\n",
    "    #print(decoder_output)\n",
    "    decoder_output = (decoder_output > prob).type(dtype=torch.float32)\n",
    "    #print(decoder_output)\n",
    "    i+=1\n",
    "    token = dataset.tensor_to_token(decoder_output)\n",
    "    \n",
    "    if previous == token:\n",
    "        repeated +=1\n",
    "        #print(repeated)\n",
    "        if repeated > 20:\n",
    "            #print(\"JEJ\")\n",
    "            #print(token)\n",
    "            token = ''.join(random.choices(previous, k = min(len(previous), 5)))\n",
    "            #print(token)\n",
    "            repeated = 0\n",
    "            previous = token\n",
    "            decoder_input = dataset.token_to_tensor(token).to(DEVICE)\n",
    "        else:\n",
    "            decoder_input = decoder_output # i ths chord is input to the next iteration\n",
    "            \n",
    "    else:\n",
    "        previous=token\n",
    "        decoder_input = decoder_output # i ths chord is input to the next iteration\n",
    "        \n",
    "    if i == 200:\n",
    "        break\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        if prob == 0.8:\n",
    "            prob = 0.7\n",
    "        else:\n",
    "            prob = 0.8\n",
    "    song += \" \" + token \n",
    "    \n",
    "    \n",
    "    #decoder_input = decoder_output # i ths chord is input to the next iteration\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4CL L L L L L L L L L L L L L L L L L L L L L@ L@ EL4@ EL4X@ GE-LQ4X@ GE-LQ49X@ G>E-SLQ49_X ]&G>Ed-SL(Q ]&G2>Ed-SL( ]&G2>Ed/-SJ J2d2- ]&G2>Ed-SJL ]&G2>Ed/-SJ ]&G2>Ed/-SJ ]&G2>Ed/-SJ B]&G2>Ed/-S B]&G2>Ed/-S B]&G2>*Ed/- Bn]&Gk2>*Ed Bn]&Gk2>*Ed Bn]&Gk2>q*6 Bn]&Gk2>q*6 Bn]&Gk2U>q* Bn]&Gk2U>q* Bn]&Gk2U>q* Bn]&Gk2U>q* Bn]&Gk2U>q* Bn]&Gk2U>q* BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q ]G>]B BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q T&B>2 BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnG2q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q ]UG2U BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q >BGnT BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q kGGT] nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q BnT]&Gk2U>q U]>2] nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q* nT]&Gk2U>q*\n"
     ]
    }
   ],
   "source": [
    "print(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = converter.MidiTxtConverter()\n",
    "\n",
    "track =c.string_to_multitrack(song.strip())\n",
    "\n",
    "pypianoroll.write(\"lstm3.mid\", track)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a2214deb2e00a4588fb64d6e2ad9e78ab07788ce628f39696990503e7a4b014"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
